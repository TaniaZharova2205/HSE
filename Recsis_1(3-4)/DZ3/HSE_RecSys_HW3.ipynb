{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ №3 Двухуровневый пайплайн\n",
    "#### В этой домашке вам предстоит написать с нуля двустадийную рекомендательную систему. \n",
    "\n",
    "#### Дата выдачи: 10.03.25\n",
    "\n",
    "#### Мягкий дедлайн: 31.03.25 23:59 MSK\n",
    "\n",
    "#### Жесткий дедлайн: 7.04.25 23:59 MSK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание\n",
    "Это творческое задание, в котором вам необходимо реализовать полный цикл построения рекомендательной системы: реализовать кандидат генераторов, придумать и собрать признаки, обучить итоговый ранкер и заинференсить модели на всех пользователей.\n",
    "\n",
    "Вам предоставляется два набора данных: `train.csv` и `test.csv` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HSE\\Recsis_1(3-4)\\DZ3\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import implicit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from gdown) (4.13.3)\n",
      "Requirement already satisfied: filelock in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from beautifulsoup4->gdown) (4.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from requests[socks]->gdown) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from requests[socks]->gdown) (2025.1.31)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/file/d/1-CcS22-UpTJeNcFlA0dVLrEQn8jnI0d-/view?usp=drive_link\n",
      "To: d:\\HSE\\Recsis_1(3-4)\\DZ3\\train.csv\n",
      "91.9kB [00:00, 3.72MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/file/d/11iz3xDh0IIoEIBY0dyRSvByY3qfiT3BG/view?usp=drive_link\n",
      "To: d:\\HSE\\Recsis_1(3-4)\\DZ3\\test.csv\n",
      "92.1kB [00:00, 3.41MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# скачиваем данные\n",
    "# если из этой ячейки не получается, то вот ссылка на папку https://drive.google.com/drive/folders/1HT0Apm8Jft0VPLJtdBBUGu9s1M7vZcoJ?usp=drive_link\n",
    "\n",
    "!pip3 install gdown\n",
    "\n",
    "\n",
    "import gdown\n",
    "# train\n",
    "url = \"https://drive.google.com/uc?id=1-CcS22-UpTJeNcFlA0dVLrEQn8jnI0d-\"\n",
    "\n",
    "output = 'train.csv'\n",
    "gdown.download(url, output, quiet=True)\n",
    "\n",
    "# test\n",
    "url = \"https://drive.google.com/uc?id=11iz3xDh0IIoEIBY0dyRSvByY3qfiT3BG\"\n",
    "\n",
    "output = 'test.csv'\n",
    "gdown.download(url, output, quiet=True)\n",
    "\n",
    "# user features\n",
    "url = \"https://drive.google.com/uc?id=1zl2jWMdUhc-IMakHlihQhJ5PGGZm9-_O\"\n",
    "output = 'users.csv'\n",
    "gdown.download(url, output, quiet=True, fuzzy=True)\n",
    "\n",
    "# item features\n",
    "url = \"https://drive.google.com/uc?id=1chCmpiCKJRjdqNftHc-t2ALl3qbAp2G8\"\n",
    "output = 'items.csv'\n",
    "gdown.download(url, output, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_csv('users.csv')\n",
    "items_df = pd.read_csv('items.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1 Этап. Модели первого уровня. (max 3 балла)\n",
    "В этом этапе вам необходимо разделить `train` датасет на 2 части: для обучения моделей первого уровня и для их валидации. Единственное условие для разбиения – разбивать нужно по времени. Данные для обучение будем называть `train_stage_1`, данные для валидации `valid_stage_1`. Объемы этих датасетов вы определяет самостоятельно. \n",
    "\n",
    "Для начала нам нужно отобрать кандидатов при помощи легких моделей. Необходимо реализовать 3 типа моделей:\n",
    "1. Любая эвристическая(алгоритмичная) модель на ваш выбор **(0.5 балл)**\n",
    "2. Любая матричная факторизация на ваш выбор **(1 балл)**\n",
    "3. Любая нейросетевая модель на ваш выбор **(1 балла)**\n",
    "\n",
    "Не забудьте использовать скор каждой модели, как признак!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждая модель должна уметь:\n",
    "1) для пары user_item предсказывать скор релевантности (масштаб скора не важен), важно обработать случаи, когда модель не можеn проскорить пользователя или айтем, вместо этого вернуть какое-то дефолтное значение\n",
    "2) для всех пользователей вернуть top-k самых релевантных айтемов (тут вам скоры не нужны)\n",
    "\n",
    "\n",
    "Дополнительно можно провести анализ кандидат генератов, измерить насколько различные айтемы они рекомендуют, например с помощью таких метрик как: [Ranked based overlap](https://github.com/changyaochen/rbo) или различные вариации [Diversity](https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/master/Base/Evaluation/metrics.py#L289). **(1 балл)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['last_watch_dt'] = pd.to_datetime(train_df.last_watch_dt)\n",
    "test_df['last_watch_dt'] = pd.to_datetime(test_df.last_watch_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2021-08-12 00:00:00')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['last_watch_dt'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование временных меток\n",
    "train_df['weekday'] = train_df.last_watch_dt.dt.weekday\n",
    "test_df['weekday'] = test_df.last_watch_dt.dt.weekday\n",
    "train_df = train_df.sort_values('last_watch_dt')\n",
    "test_df = test_df.sort_values('last_watch_dt')\n",
    "\n",
    "\n",
    "train_stage_1 = train_df.loc[(train_df.last_watch_dt < '2021-08-06')].copy()\n",
    "valid_stage_1 = train_df.loc[train_df.last_watch_dt >= '2021-08-06'].copy()\n",
    "\n",
    "user_ids = train_df['user_id'].unique()\n",
    "item_ids = train_df['item_id'].unique()\n",
    "user_to_idx = {user: idx for idx, user in enumerate(user_ids)}\n",
    "idx_to_user = {idx: user for user, idx in user_to_idx.items()}\n",
    "item_to_idx = {item: idx for idx, item in enumerate(item_ids)}\n",
    "idx_to_item = {idx: item for item, idx in item_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((608467, 7), (4473822, 7), (393134, 7))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape, train_stage_1.shape, valid_stage_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Эвристическая модель (популярность по дням недели)\n",
    "class HeuristicModel:\n",
    "    def __init__(self):\n",
    "        self.popularity = defaultdict(dict)\n",
    "        self.default_score = 0.5\n",
    "        \n",
    "    def fit(self, df):\n",
    "        for weekday in range(7):\n",
    "            weekday_data = df[df['weekday'] == weekday]\n",
    "            item_counts = weekday_data['item_id'].value_counts()\n",
    "            total = item_counts.sum()\n",
    "            for item, count in item_counts.items():\n",
    "                self.popularity[weekday][item] = count / total\n",
    "    \n",
    "    def predict(self, user_id, item_id, weekday=None, default=None):\n",
    "        if weekday is None:\n",
    "            weekday = np.random.randint(0, 7)\n",
    "            \n",
    "        if item_id in self.popularity[weekday]:\n",
    "            return self.popularity[weekday][item_id]\n",
    "        return default if default is not None else self.default_score\n",
    "    \n",
    "    def recommend(self, user_id, k=10, weekday=None):\n",
    "        if weekday is None:\n",
    "            weekday = np.random.randint(0, 7)\n",
    "            \n",
    "        popular_items = sorted(self.popularity[weekday].items(), \n",
    "                             key=lambda x: -x[1])\n",
    "        return [item for item, score in popular_items[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Матричная факторизация (implicit ALS)\n",
    "class ImplicitFactorizationModel:\n",
    "    def __init__(self, factors=50, iterations=15, regularization=0.01):\n",
    "        self.model = implicit.als.AlternatingLeastSquares(\n",
    "            factors=factors,\n",
    "            iterations=iterations,\n",
    "            regularization=regularization,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.default_score = 0.5\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        \n",
    "    def fit(self, df):\n",
    "        # Создаем разреженную матрицу взаимодействий\n",
    "        user_indices = df['user_id'].map(user_to_idx).values\n",
    "        item_indices = df['item_id'].map(item_to_idx).values\n",
    "        durations = df['total_dur'].values\n",
    "        \n",
    "        # Нормализуем продолжительности\n",
    "        durations = durations / durations.max()\n",
    "        \n",
    "        # Создаем матрицу в формате COO\n",
    "        from scipy.sparse import coo_matrix\n",
    "        interactions = coo_matrix(\n",
    "            (durations, (user_indices, item_indices)),\n",
    "            shape=(len(user_to_idx), len(item_to_idx))\n",
    "        )\n",
    "        \n",
    "        # Обучаем модель\n",
    "        self.model.fit(interactions)\n",
    "        self.user_factors = self.model.user_factors\n",
    "        self.item_factors = self.model.item_factors\n",
    "        \n",
    "    def predict(self, user_id, item_id, default=None):\n",
    "        if user_id not in user_to_idx or item_id not in item_to_idx:\n",
    "            return default if default is not None else self.default_score\n",
    "            \n",
    "        user_idx = user_to_idx[user_id]\n",
    "        item_idx = item_to_idx[item_id]\n",
    "        return self.user_factors[user_idx] @ self.item_factors[item_idx].T\n",
    "    \n",
    "    def recommend(self, user_id, k=10):\n",
    "        if user_id not in user_to_idx:\n",
    "            return []\n",
    "            \n",
    "        user_idx = user_to_idx[user_id]\n",
    "        items, scores = self.model.recommend(\n",
    "            user_idx, \n",
    "            user_items=None, \n",
    "            N=k, \n",
    "            filter_already_liked_items=False\n",
    "        )\n",
    "        return [idx_to_item[item] for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Нейросетевая модель\n",
    "class InteractionDataset(Dataset):\n",
    "    def __init__(self, df, user_to_idx, item_to_idx):\n",
    "        self.df = df.copy()\n",
    "        self.df['user_idx'] = self.df['user_id'].map(user_to_idx)\n",
    "        self.df['item_idx'] = self.df['item_id'].map(item_to_idx)\n",
    "        self.df = self.df.dropna(subset=['user_idx', 'item_idx'])\n",
    "        \n",
    "        # Нормализация target\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.df['target'] = self.scaler.fit_transform(\n",
    "            self.df[['total_dur']]\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return (\n",
    "            torch.LongTensor([row['user_idx']]),\n",
    "            torch.LongTensor([row['item_idx']]),\n",
    "            torch.FloatTensor([row['target']])\n",
    "        )\n",
    "    \n",
    "class NeuralCF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim=32):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, emb_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, emb_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        user_emb = self.user_embedding(user).squeeze(1)\n",
    "        item_emb = self.item_embedding(item).squeeze(1)\n",
    "        x = torch.cat([user_emb, item_emb], dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class TorchNeuralModel:\n",
    "    def __init__(self, emb_dim=32, lr=0.01, batch_size=1024, n_epochs=3):\n",
    "        self.model = None\n",
    "        self.emb_dim = emb_dim\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.default_score = 0.5\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "    def fit(self, df):\n",
    "        # Подготовка данных\n",
    "        dataset = InteractionDataset(df, user_to_idx, item_to_idx)\n",
    "        self.scaler = dataset.scaler\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        # Инициализация модели\n",
    "        n_users = len(user_to_idx)\n",
    "        n_items = len(item_to_idx)\n",
    "        self.model = NeuralCF(n_users, n_items, self.emb_dim)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Обучение\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for user, item, target in tqdm(dataloader, desc=f'Epoch {epoch+1}/{self.n_epochs}', leave=False):\n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(user, item)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    def predict(self, user_id, item_id, default=None):\n",
    "        if user_id not in user_to_idx or item_id not in item_to_idx:\n",
    "            return default if default is not None else self.default_score\n",
    "            \n",
    "        user_idx = torch.LongTensor([user_to_idx[user_id]])\n",
    "        item_idx = torch.LongTensor([item_to_idx[item_id]])\n",
    "        with torch.no_grad():\n",
    "            score = self.model(user_idx, item_idx).item()\n",
    "        return score\n",
    "    \n",
    "    def recommend(self, user_id, k=10):\n",
    "        if user_id not in user_to_idx:\n",
    "            return []\n",
    "            \n",
    "        user_idx = user_to_idx[user_id]\n",
    "        user_tensor = torch.LongTensor([user_idx] * len(item_to_idx))\n",
    "        item_tensor = torch.LongTensor(list(item_to_idx.values()))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            scores = self.model(user_tensor, item_tensor).flatten().numpy()\n",
    "        \n",
    "        top_items = np.argsort(-scores)[:k]\n",
    "        return [idx_to_item[item] for item in top_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Heuristic Model...\n",
      "Training Implicit ALS Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HSE\\Recsis_1(3-4)\\DZ3\\.venv\\lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.31058335304260254 seconds\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:53<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PyTorch Neural Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    }
   ],
   "source": [
    "# Инициализация и обучение моделей\n",
    "print(\"Training Heuristic Model...\")\n",
    "heuristic_model = HeuristicModel()\n",
    "heuristic_model.fit(train_stage_1)\n",
    "print(\"Training Implicit ALS Model...\")\n",
    "implicit_model = ImplicitFactorizationModel(factors=50, iterations=15)\n",
    "implicit_model.fit(train_stage_1)\n",
    "print(\"Training PyTorch Neural Model...\")\n",
    "torch_model = TorchNeuralModel(emb_dim=32, n_epochs=3, batch_size=1024)\n",
    "torch_model.fit(train_stage_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Recommendation Diversity...\n",
      "Pairwise Overlap Matrix:\n",
      "[[1.000e+00 2.156e-01 0.000e+00]\n",
      " [2.156e-01 1.000e+00 2.000e-04]\n",
      " [0.000e+00 2.000e-04 1.000e+00]]\n",
      "\n",
      "Mean Diversity: 0.928\n"
     ]
    }
   ],
   "source": [
    "# Функция для оценки разнообразия рекомендаций\n",
    "def evaluate_diversity(models, users, k=10):\n",
    "    overlaps = np.zeros((len(models), len(models)))\n",
    "    diversities = []\n",
    "    \n",
    "    for user in users[:1000]: \n",
    "        recommendations = []\n",
    "        for model in models:\n",
    "            try:\n",
    "                recs = set(model.recommend(user, k=k))\n",
    "                recommendations.append(recs)\n",
    "            except:\n",
    "                recommendations.append(set())\n",
    "        \n",
    "        # Рассчитываем попарные пересечения\n",
    "        for i in range(len(models)):\n",
    "            for j in range(len(models)):\n",
    "                overlaps[i,j] += len(recommendations[i] & recommendations[j]) / k\n",
    "                \n",
    "        # Рассчитываем diversity для одного пользователя\n",
    "        all_recs = set()\n",
    "        for rec in recommendations:\n",
    "            all_recs.update(rec)\n",
    "        diversity = len(all_recs) / (len(models) * k)\n",
    "        diversities.append(diversity)\n",
    "    \n",
    "    overlaps /= min(1000, len(users))\n",
    "    mean_diversity = np.mean(diversities)\n",
    "    \n",
    "    print(\"Pairwise Overlap Matrix:\")\n",
    "    print(overlaps)\n",
    "    print(f\"\\nMean Diversity: {mean_diversity:.3f}\")\n",
    "    \n",
    "    return overlaps, mean_diversity\n",
    "\n",
    "# Оценка разнообразия рекомендаций\n",
    "print(\"\\nEvaluating Recommendation Diversity...\")\n",
    "models = [heuristic_model, implicit_model, torch_model]\n",
    "sample_users = valid_stage_1['user_id'].unique()[:1000]\n",
    "overlaps, diversity = evaluate_diversity(models, sample_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2 Этап. Генерация и сборка признаков. (max 2 балла)\n",
    "Необходимо собрать минимум 10 осмысленных (`np.radndom.rand()` не подойдет) признаков, при этом:\n",
    "1. 2 должны относиться только к сущности \"пользователь\" (например средний % просмотра фильмов у этой возрастной категории)\n",
    "2. 2 должны относиться только к сущности \"айтем\" (например средний средний % просмотра данного фильма)\n",
    "3. 6 признаков, которые показывают связь пользователя и айтема (например средний % просмотра фильмов с данным актером (айтем) у пользователей с таким же полом (пользователь)). \n",
    "\n",
    "### ВАЖНО!  \n",
    "\n",
    "1. **В датасете есть колонка `watched_prct`. Ее можно использовать для генерации признаков (например сколько пользователь в среднем смотрит фильмы), но нельзя подавать в модель, как отдельную фичу, потому что она напрямую связана с target.**\n",
    "2. **Все признаки должны быть собраны без дата лика, то есть если пользователь посмотрел фильм 10 августа, то признаки мы можем считать только на данных до 9 августа включительно.**\n",
    "\n",
    "\n",
    "### Разбалловка\n",
    "Обучение ранкера будет проходить на `valid_stage_1`, как  раз на которой мы валидировали модели, а тестировать на `test`. Поэтому есть 2 варианта сборки признаков, **реализовать нужно только 1 из них:**\n",
    "1. Для обучения собираем признаки на первый день `valid_stage_1`, а для теста на первый день `test`. Например, если `valid_stage_1` начинается 5 сентября, то все признаки мы можем собирать только по 4 сентября включительно. **(1 балл)**\n",
    "2. Признаки будем собирать честно на каждый день, то есть на 5 сентября собираем с начала до 4, на 6 сентября с начала до 5 и т.д. **(2 балла)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сначала подготовим основные таблицы\n",
    "train_df['last_watch_dt'] = pd.to_datetime(train_df['last_watch_dt'])\n",
    "train_df = train_df.sort_values('last_watch_dt')\n",
    "\n",
    "# Создадим функции для генерации признаков\n",
    "def generate_features(df, users_df, items_df, date_cutoff):\n",
    "    df_copy = df.copy()\n",
    "    history = df[df['last_watch_dt'] < date_cutoff]\n",
    "\n",
    "    # Признаки пользователя\n",
    "    # 1. Средний % досмотра у пользователя\n",
    "    user_avg_watch = history.groupby('user_id')['watched_pct'].mean().rename('user_avg_watch')\n",
    "    # 2. Количество просмотров у пользователя\n",
    "    user_watch_count = history.groupby('user_id')['item_id'].count().rename('user_watch_count')\n",
    "\n",
    "    # Признаки айтема\n",
    "    # 1. Средний % досмотра у айтема\n",
    "    item_avg_watch = history.groupby('item_id')['watched_pct'].mean().rename('item_avg_watch')\n",
    "    # 2. Количество просмотров у айтема\n",
    "    item_watch_count = history.groupby('item_id')['user_id'].count().rename('item_watch_count')\n",
    "\n",
    "    # Признаки взаимодействия\n",
    "\n",
    "    # 1. Средний % досмотра для жанров, которые смотрел пользователь\n",
    "    items_genre = items_df[['item_id', 'genres']].dropna()\n",
    "    items_genre['genres'] = items_genre['genres'].apply(lambda x: x.split(',')[0])  # возьмем только первый жанр\n",
    "    history = history.merge(items_genre, on='item_id', how='left')\n",
    "    user_genre_avg = history.groupby(['user_id', 'genres'])['watched_pct'].mean().rename('user_genre_avg').reset_index()\n",
    "\n",
    "    # 2. Средний % досмотра пользователями с тем же полом на этот жанр\n",
    "    history = history.merge(users_df[['user_id', 'sex']], on='user_id', how='left')\n",
    "    genre_sex_avg = history.groupby(['genres', 'sex'])['watched_pct'].mean().rename('genre_sex_avg').reset_index()\n",
    "\n",
    "    # 3. Средний % досмотра по странам (item)\n",
    "    items_country = items_df[['item_id', 'countries']].dropna()\n",
    "    items_country['countries'] = items_country['countries'].apply(lambda x: x.split(',')[0])\n",
    "    history = history.merge(items_country, on='item_id', how='left')\n",
    "    user_country_avg = history.groupby(['user_id', 'countries'])['watched_pct'].mean().rename('user_country_avg').reset_index()\n",
    "\n",
    "    # 4. Средний % досмотра по типу контента (movie/show)\n",
    "    items_type = items_df[['item_id', 'content_type']]\n",
    "    history = history.merge(items_type, on='item_id', how='left')\n",
    "    user_content_type_avg = history.groupby(['user_id', 'content_type'])['watched_pct'].mean().rename('user_content_type_avg').reset_index()\n",
    "\n",
    "    # 5. Средний % досмотра для фильмов, выпущенных до 2010 у пользователя\n",
    "    items_year = items_df[['item_id', 'release_year']]\n",
    "    history = history.merge(items_year, on='item_id', how='left')\n",
    "    history['old_movie'] = history['release_year'] < 2010\n",
    "    user_old_movie_avg = history.groupby(['user_id', 'old_movie'])['watched_pct'].mean().rename('user_old_movie_avg').reset_index()\n",
    "\n",
    "    # 6. Средний % досмотра фильмов для детей у пользователя\n",
    "    items_forkids = items_df[['item_id', 'for_kids']]\n",
    "    history = history.merge(items_forkids, on='item_id', how='left')\n",
    "    user_forkids_avg = history.groupby(['user_id'])['for_kids'].mean().rename('user_forkids_avg')\n",
    "\n",
    "    # Собираем валидационный сет на конкретную дату\n",
    "    current_data = df[df['last_watch_dt'] == date_cutoff].copy()\n",
    "\n",
    "    # Мержим все признаки\n",
    "    current_data = current_data.merge(user_avg_watch, on='user_id', how='left')\n",
    "    current_data = current_data.merge(user_watch_count, on='user_id', how='left')\n",
    "    current_data = current_data.merge(item_avg_watch, on='item_id', how='left')\n",
    "    current_data = current_data.merge(item_watch_count, on='item_id', how='left')\n",
    "    current_data = current_data.merge(user_forkids_avg, on='user_id', how='left')\n",
    "\n",
    "    current_data = current_data.merge(items_genre, on='item_id', how='left')\n",
    "    current_data = current_data.merge(user_genre_avg, on=['user_id', 'genres'], how='left')\n",
    "    current_data = current_data.merge(users_df[['user_id', 'sex']], on='user_id', how='left')\n",
    "    current_data = current_data.merge(genre_sex_avg, on=['genres', 'sex'], how='left')\n",
    "    current_data = current_data.merge(items_country, on='item_id', how='left')\n",
    "    current_data = current_data.merge(user_country_avg, on=['user_id', 'countries'], how='left')\n",
    "    current_data = current_data.merge(items_type, on='item_id', how='left')\n",
    "    current_data = current_data.merge(user_content_type_avg, on=['user_id', 'content_type'], how='left')\n",
    "    current_data = current_data.merge(items_year, on='item_id', how='left')\n",
    "    current_data['old_movie'] = current_data['release_year'] < 2010\n",
    "    current_data = current_data.merge(user_old_movie_avg, on=['user_id', 'old_movie'], how='left')\n",
    "\n",
    "    return current_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [17:40<00:00,  6.93s/it]\n",
      "100%|██████████| 10/10 [00:14<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "# Генерация признаков для train и test\n",
    "def generate_features_for_dates(df, users_df, items_df):\n",
    "    dates = df['last_watch_dt'].dt.date.unique()\n",
    "    features = []\n",
    "    for date in tqdm(dates):\n",
    "        date = pd.Timestamp(date)\n",
    "        featured = generate_features(df, users_df, items_df, date)\n",
    "        features.append(featured)\n",
    "    return pd.concat(features)\n",
    "train_df_with_features = generate_features_for_dates(train_df, users_df, items_df)\n",
    "test_df_with_features = generate_features_for_dates(test_df, users_df, items_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>last_watch_dt</th>\n",
       "      <th>total_dur</th>\n",
       "      <th>watched_pct</th>\n",
       "      <th>target</th>\n",
       "      <th>weekday</th>\n",
       "      <th>user_avg_watch</th>\n",
       "      <th>user_watch_count</th>\n",
       "      <th>item_avg_watch</th>\n",
       "      <th>...</th>\n",
       "      <th>user_genre_avg</th>\n",
       "      <th>sex</th>\n",
       "      <th>genre_sex_avg</th>\n",
       "      <th>countries</th>\n",
       "      <th>user_country_avg</th>\n",
       "      <th>content_type</th>\n",
       "      <th>user_content_type_avg</th>\n",
       "      <th>release_year</th>\n",
       "      <th>old_movie</th>\n",
       "      <th>user_old_movie_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56492</th>\n",
       "      <td>235493</td>\n",
       "      <td>11756</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>6499</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>44.666667</td>\n",
       "      <td>9.0</td>\n",
       "      <td>54.374329</td>\n",
       "      <td>...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Ж</td>\n",
       "      <td>57.461850</td>\n",
       "      <td>США</td>\n",
       "      <td>21.250000</td>\n",
       "      <td>film</td>\n",
       "      <td>44.666667</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>True</td>\n",
       "      <td>16.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56493</th>\n",
       "      <td>134400</td>\n",
       "      <td>341</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>417</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>39.333333</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.521054</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Россия</td>\n",
       "      <td>46.444444</td>\n",
       "      <td>series</td>\n",
       "      <td>54.333333</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>False</td>\n",
       "      <td>39.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56494</th>\n",
       "      <td>598090</td>\n",
       "      <td>15297</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>12120</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.358940</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ж</td>\n",
       "      <td>45.714737</td>\n",
       "      <td>Россия</td>\n",
       "      <td>NaN</td>\n",
       "      <td>series</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56495</th>\n",
       "      <td>889540</td>\n",
       "      <td>14899</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>5085</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>68.789474</td>\n",
       "      <td>19.0</td>\n",
       "      <td>33.878127</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Ж</td>\n",
       "      <td>45.981279</td>\n",
       "      <td>США</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>film</td>\n",
       "      <td>70.882353</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>False</td>\n",
       "      <td>71.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56496</th>\n",
       "      <td>99479</td>\n",
       "      <td>3427</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>5758</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40.007477</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ж</td>\n",
       "      <td>49.238777</td>\n",
       "      <td>Россия</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>series</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>False</td>\n",
       "      <td>31.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id last_watch_dt  total_dur  watched_pct  target  \\\n",
       "56492   235493    11756    2021-08-12       6499        100.0       1   \n",
       "56493   134400      341    2021-08-12        417          8.0       0   \n",
       "56494   598090    15297    2021-08-12      12120         75.0       1   \n",
       "56495   889540    14899    2021-08-12       5085        100.0       1   \n",
       "56496    99479     3427    2021-08-12       5758         25.0       0   \n",
       "\n",
       "       weekday  user_avg_watch  user_watch_count  item_avg_watch  ...  \\\n",
       "56492        3       44.666667               9.0       54.374329  ...   \n",
       "56493        3       39.333333              12.0        5.521054  ...   \n",
       "56494        3             NaN               NaN       55.358940  ...   \n",
       "56495        3       68.789474              19.0       33.878127  ...   \n",
       "56496        3       31.600000               5.0       40.007477  ...   \n",
       "\n",
       "       user_genre_avg  sex genre_sex_avg  countries user_country_avg  \\\n",
       "56492            51.0    Ж     57.461850        США        21.250000   \n",
       "56493             NaN  NaN           NaN     Россия        46.444444   \n",
       "56494             NaN    Ж     45.714737     Россия              NaN   \n",
       "56495            10.0    Ж     45.981279        США        47.000000   \n",
       "56496             NaN    Ж     49.238777     Россия        31.600000   \n",
       "\n",
       "       content_type user_content_type_avg  release_year old_movie  \\\n",
       "56492          film             44.666667        1997.0      True   \n",
       "56493        series             54.333333        2021.0     False   \n",
       "56494        series                   NaN        2021.0     False   \n",
       "56495          film             70.882353        2021.0     False   \n",
       "56496        series             21.000000        2020.0     False   \n",
       "\n",
       "       user_old_movie_avg  \n",
       "56492           16.500000  \n",
       "56493           39.333333  \n",
       "56494                 NaN  \n",
       "56495           71.153846  \n",
       "56496           31.600000  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_with_features.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>last_watch_dt</th>\n",
       "      <th>total_dur</th>\n",
       "      <th>watched_pct</th>\n",
       "      <th>target</th>\n",
       "      <th>weekday</th>\n",
       "      <th>user_avg_watch</th>\n",
       "      <th>user_watch_count</th>\n",
       "      <th>item_avg_watch</th>\n",
       "      <th>...</th>\n",
       "      <th>user_genre_avg</th>\n",
       "      <th>sex</th>\n",
       "      <th>genre_sex_avg</th>\n",
       "      <th>countries</th>\n",
       "      <th>user_country_avg</th>\n",
       "      <th>content_type</th>\n",
       "      <th>user_content_type_avg</th>\n",
       "      <th>release_year</th>\n",
       "      <th>old_movie</th>\n",
       "      <th>user_old_movie_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71166</th>\n",
       "      <td>795768</td>\n",
       "      <td>13211</td>\n",
       "      <td>2021-08-22</td>\n",
       "      <td>77</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.210526</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Россия</td>\n",
       "      <td>NaN</td>\n",
       "      <td>film</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71167</th>\n",
       "      <td>793910</td>\n",
       "      <td>6162</td>\n",
       "      <td>2021-08-22</td>\n",
       "      <td>322</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>68.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>50.498048</td>\n",
       "      <td>...</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>Ж</td>\n",
       "      <td>34.180118</td>\n",
       "      <td>Испания</td>\n",
       "      <td>NaN</td>\n",
       "      <td>film</td>\n",
       "      <td>68.666667</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>False</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71168</th>\n",
       "      <td>418605</td>\n",
       "      <td>9151</td>\n",
       "      <td>2021-08-22</td>\n",
       "      <td>270</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>23.637931</td>\n",
       "      <td>58.0</td>\n",
       "      <td>56.454545</td>\n",
       "      <td>...</td>\n",
       "      <td>45.500000</td>\n",
       "      <td>Ж</td>\n",
       "      <td>37.972520</td>\n",
       "      <td>Великобритания</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>film</td>\n",
       "      <td>23.941176</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>False</td>\n",
       "      <td>23.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71169</th>\n",
       "      <td>54946</td>\n",
       "      <td>8242</td>\n",
       "      <td>2021-08-22</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>12.0</td>\n",
       "      <td>19.898990</td>\n",
       "      <td>...</td>\n",
       "      <td>30.142857</td>\n",
       "      <td>Ж</td>\n",
       "      <td>34.180118</td>\n",
       "      <td>США</td>\n",
       "      <td>35.666667</td>\n",
       "      <td>film</td>\n",
       "      <td>32.727273</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>True</td>\n",
       "      <td>61.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71170</th>\n",
       "      <td>804</td>\n",
       "      <td>14282</td>\n",
       "      <td>2021-08-22</td>\n",
       "      <td>4061</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>35.625000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>36.337349</td>\n",
       "      <td>...</td>\n",
       "      <td>36.833333</td>\n",
       "      <td>М</td>\n",
       "      <td>41.381618</td>\n",
       "      <td>Россия</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>film</td>\n",
       "      <td>35.625000</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>True</td>\n",
       "      <td>36.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id last_watch_dt  total_dur  watched_pct  target  \\\n",
       "71166   795768    13211    2021-08-22         77          1.0       0   \n",
       "71167   793910     6162    2021-08-22        322          6.0       0   \n",
       "71168   418605     9151    2021-08-22        270          4.0       0   \n",
       "71169    54946     8242    2021-08-22         24          0.0       0   \n",
       "71170      804    14282    2021-08-22       4061         66.0       1   \n",
       "\n",
       "       weekday  user_avg_watch  user_watch_count  item_avg_watch  ...  \\\n",
       "71166        6             NaN               NaN       31.210526  ...   \n",
       "71167        6       68.666667               3.0       50.498048  ...   \n",
       "71168        6       23.637931              58.0       56.454545  ...   \n",
       "71169        6       30.000000              12.0       19.898990  ...   \n",
       "71170        6       35.625000               8.0       36.337349  ...   \n",
       "\n",
       "       user_genre_avg  sex genre_sex_avg       countries user_country_avg  \\\n",
       "71166             NaN  NaN           NaN          Россия              NaN   \n",
       "71167      100.000000    Ж     34.180118         Испания              NaN   \n",
       "71168       45.500000    Ж     37.972520  Великобритания        24.000000   \n",
       "71169       30.142857    Ж     34.180118             США        35.666667   \n",
       "71170       36.833333    М     41.381618          Россия        44.000000   \n",
       "\n",
       "       content_type user_content_type_avg  release_year old_movie  \\\n",
       "71166          film                   NaN        1997.0      True   \n",
       "71167          film             68.666667        2013.0     False   \n",
       "71168          film             23.941176        2016.0     False   \n",
       "71169          film             32.727273        1991.0      True   \n",
       "71170          film             35.625000        2009.0      True   \n",
       "\n",
       "       user_old_movie_avg  \n",
       "71166                 NaN  \n",
       "71167          100.000000  \n",
       "71168           23.111111  \n",
       "71169           61.250000  \n",
       "71170           36.833333  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_with_features.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stage_with_features_1 = train_df_with_features.loc[(train_df_with_features.last_watch_dt < '2021-08-06')].copy()\n",
    "valid_stage_with_features_1 = train_df_with_features.loc[train_df_with_features.last_watch_dt >= '2021-08-06'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3 Этап. Обучение финального ранкера (max 2 балла)\n",
    "Собрав все признаки из этапа 2, добавив скоры моделей из этапа 1 для каждой пары пользователь-айтем (где это возможно), пришло время обучать ранкер. В качестве ранкера можно использовать либо [xgboost](https://xgboost.readthedocs.io/en/stable/) или [catboost](https://catboost.ai/). Обучать можно как `Classfier`, так и `Ranker`, выбираем то, что лучше сработает. Обучение ранкера будет проходить на `valid_stage_1`, как  раз на которой мы валидировали модели, а тестировать на `test`, которую мы до сих пор не трогали.  Заметьте, что у нас в тесте есть холодные пользователи – те, кого не было в train и активные – те, кто был в train. Возможно их стоит обработать по отдельности (а может и нет).  \n",
    "(1 балл)\n",
    "\n",
    "После получения лучшей модели надо посмотреть на важность признаков и [shap values](https://shap.readthedocs.io/en/latest/index.html), чтобы:\n",
    "1. Интерпритировать признаки, которые вы собрали, насколько они полезные\n",
    "2. Проверить наличие ликов – если важность фичи в 100 раз больше, чем у всех остальных, то явно что-то не то  \n",
    "\n",
    "(1 балл)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка данных для обучения\n",
    "def prepare_data(train_data, test_data):\n",
    "    # Выбираем только нужные колонки (исключаем watched_pct и другие ликующие признаки)\n",
    "    features = [col for col in train_data.columns if col not in ['user_id', 'item_id', 'last_watch_dt', \n",
    "                                                               'watched_pct', 'target', 'weekday']]\n",
    "    \n",
    "    X_train = train_data[features].fillna(0)\n",
    "    y_train = train_data['target']\n",
    "    \n",
    "    X_test = test_data[features].fillna(0)\n",
    "    y_test = test_data['target']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, features\n",
    "\n",
    "# Подготовка данных\n",
    "X_train, y_train, X_valid, y_valid, feature_names = prepare_data(\n",
    "    train_stage_with_features_1,\n",
    "    valid_stage_with_features_1\n",
    ")\n",
    "\n",
    "# Разделение на холодных и теплых пользователей\n",
    "def split_cold_warm(data, train_users):\n",
    "    warm_mask = data['user_id'].isin(train_users)\n",
    "    return data[warm_mask], data[~warm_mask]\n",
    "\n",
    "train_users = train_stage_with_features_1['user_id'].unique()\n",
    "valid_warm, valid_cold = split_cold_warm(valid_stage_with_features_1, train_users)\n",
    "\n",
    "X_valid_warm = valid_warm[feature_names].fillna(0)\n",
    "y_valid_warm = valid_warm['target']\n",
    "\n",
    "# Обучение XGBoost Ranker\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=feature_names)\n",
    "dvalid = xgb.DMatrix(X_valid_warm, label=y_valid_warm, feature_names=feature_names)\n",
    "\n",
    "params = {\n",
    "    'objective': 'rank:ndcg',\n",
    "    'eval_metric': 'ndcg@5',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 6,\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "evals = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "model = xgb.train(params, dtrain, num_boost_round=1000, \n",
    "                 evals=evals, early_stopping_rounds=50, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка на тестовых данных\n",
    "test_warm, test_cold = split_cold_warm(test_df_with_features, train_users)\n",
    "X_test_warm = test_warm[feature_names].fillna(0)\n",
    "y_test_warm = test_warm['target']\n",
    "\n",
    "dtest = xgb.DMatrix(X_test_warm, feature_names=feature_names)\n",
    "test_preds = model.predict(dtest)\n",
    "\n",
    "# Метрика NDCG\n",
    "print(f\"NDCG@5 на тесте: {ndcg_score([y_test_warm], [test_preds], k=5)}\")\n",
    "\n",
    "# Анализ важности признаков\n",
    "importance = model.get_score(importance_type='weight')\n",
    "importance = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nТоп-10 важных признаков:\")\n",
    "for feat, score in importance[:10]:\n",
    "    print(f\"{feat}: {score}\")\n",
    "\n",
    "# SHAP анализ\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_train.iloc[:1000])\n",
    "\n",
    "shap.summary_plot(shap_values, X_train.iloc[:1000], feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стратегия для холодных пользователей - используем популярные айтемы\n",
    "if len(test_cold) > 0:\n",
    "    # Вычисляем популярность айтемов в трейне\n",
    "    item_popularity = train_stage_with_features_1.groupby('item_id')['target'].mean()\n",
    "    \n",
    "    # Сортируем айтемы по популярности\n",
    "    top_items = item_popularity.sort_values(ascending=False).head(100).index\n",
    "    \n",
    "    # Для каждого холодного пользователя рекомендуем топовые айтемы\n",
    "    test_cold_recommendations = []\n",
    "    for user_id in test_cold['user_id'].unique():\n",
    "        for item_id in top_items:\n",
    "            test_cold_recommendations.append({'user_id': user_id, 'item_id': item_id, 'pred': 1.0})\n",
    "    \n",
    "    test_cold_preds = pd.DataFrame(test_cold_recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4 Этап. Инференс лучшего ранкера (max 3 балла)\n",
    "\n",
    "Теперь мы хотим построить рекомендации \"на завтра\", для этого нам нужно:\n",
    "\n",
    "1. Обучить модели первого уровня на всех (train+test) данных (0.5 балла)\n",
    "2. Для каждой модели первого уровня для каждого пользователя сгененировать N кандидатов (0.5 балла)\n",
    "3. \"Склеить\" всех кандидатов для каждого пользователя (дубли выкинуть), посчитать скоры от всех моделей (0.5 балла)\n",
    "4. Собрать фичи для ваших кандидатов (теперь можем считать признаки на всех данных) (0.5 балла)\n",
    "5. Проскорить всех кандидатов бустингом и оставить k лучших (0.5 балла)\n",
    "6. Посчитать разнообразие(Diversity) и построить график от Diversity(k) (0.5 балла)\n",
    "\n",
    "\n",
    "Все гиперпараметры (N, k) определяете только Вы!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
