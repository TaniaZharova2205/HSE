{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ №3 Двухуровневый пайплайн\n",
    "#### В этой домашке вам предстоит написать с нуля двустадийную рекомендательную систему. \n",
    "\n",
    "#### Дата выдачи: 10.03.25\n",
    "\n",
    "#### Мягкий дедлайн: 31.03.25 23:59 MSK\n",
    "\n",
    "#### Жесткий дедлайн: 7.04.25 23:59 MSK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание\n",
    "Это творческое задание, в котором вам необходимо реализовать полный цикл построения рекомендательной системы: реализовать кандидат генераторов, придумать и собрать признаки, обучить итоговый ранкер и заинференсить модели на всех пользователей.\n",
    "\n",
    "Вам предоставляется два набора данных: `train.csv` и `test.csv` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting beautifulsoup4 (from gdown)\n",
      "  Using cached beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting filelock (from gdown)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests[socks] in d:\\dz_recsys\\.venv\\lib\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in d:\\dz_recsys\\.venv\\lib\\site-packages (from gdown) (4.67.1)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\dz_recsys\\.venv\\lib\\site-packages (from beautifulsoup4->gdown) (4.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\dz_recsys\\.venv\\lib\\site-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\dz_recsys\\.venv\\lib\\site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\dz_recsys\\.venv\\lib\\site-packages (from requests[socks]->gdown) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\dz_recsys\\.venv\\lib\\site-packages (from requests[socks]->gdown) (2025.1.31)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: colorama in d:\\dz_recsys\\.venv\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Using cached beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, PySocks, filelock, beautifulsoup4, gdown\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.13.3 filelock-3.18.0 gdown-5.2.0 soupsieve-2.6\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/file/d/1-CcS22-UpTJeNcFlA0dVLrEQn8jnI0d-/view?usp=drive_link\n",
      "To: d:\\DZ_RECSYS\\train.csv\n",
      "34.3kB [00:00, 15.7MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/file/d/11iz3xDh0IIoEIBY0dyRSvByY3qfiT3BG/view?usp=drive_link\n",
      "To: d:\\DZ_RECSYS\\test.csv\n",
      "91.1kB [00:00, 3.27MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# скачиваем данные\n",
    "# если из этой ячейки не получается, то вот ссылка на папку https://drive.google.com/drive/folders/1HT0Apm8Jft0VPLJtdBBUGu9s1M7vZcoJ?usp=drive_link\n",
    "\n",
    "!pip3 install gdown\n",
    "\n",
    "\n",
    "import gdown\n",
    "# train\n",
    "url = \"https://drive.google.com/file/d/1-CcS22-UpTJeNcFlA0dVLrEQn8jnI0d-/view?usp=drive_link\"\n",
    "output = 'train.csv'\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "# test\n",
    "url = \"https://drive.google.com/file/d/11iz3xDh0IIoEIBY0dyRSvByY3qfiT3BG/view?usp=drive_link\"\n",
    "output = 'test.csv'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1 Этап. Модели первого уровня. (max 3 балла)\n",
    "В этом этапе вам необходимо разделить `train` датасет на 2 части: для обучения моделей первого уровня и для их валидации. Единственное условие для разбиения – разбивать нужно по времени. Данные для обучение будем называть `train_stage_1`, данные для валидации `valid_stage_1`. Объемы этих датасетов вы определяет самостоятельно. \n",
    "\n",
    "Для начала нам нужно отобрать кандидатов при помощи легких моделей. Необходимо реализовать 3 типа моделей:\n",
    "1. Любая эвристическая(алгоритмичная) модель на ваш выбор **(0.5 балл)**\n",
    "2. Любая матричная факторизация на ваш выбор **(1 балл)**\n",
    "3. Любая нейросетевая модель на ваш выбор **(1 балла)**\n",
    "\n",
    "Не забудьте использовать скор каждой модели, как признак!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-08-12'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "train_df.last_watch_dt.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of users which are included both in train/test data:  52311\n"
     ]
    }
   ],
   "source": [
    "# Преобразование временных меток\n",
    "train_df['weekday'] = pd.to_datetime(train_df.last_watch_dt).dt.weekday\n",
    "test_df['weekday'] = pd.to_datetime(test_df.last_watch_dt).dt.weekday\n",
    "train_df = train_df.sort_values('last_watch_dt')\n",
    "\n",
    "train_stage_1 = train_df.loc[(train_df.last_watch_dt < '2021-08-06')].copy()\n",
    "valid_stage_1 = train_df.loc[train_df.last_watch_dt >= '2021-08-06'].copy()\n",
    "\n",
    "train_users = train_stage_1.user_id.unique()\n",
    "valid_users = valid_stage_1.user_id.unique()\n",
    "test_users = test_df.user_id.unique()\n",
    "\n",
    "\n",
    "all_included = np.intersect1d(np.intersect1d(valid_users, train_users), test_users)\n",
    "\n",
    "print('number of users which are included both in train/test data: ', all_included.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((608467, 7), (4473822, 7), (393134, 7))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape, train_stage_1.shape, valid_stage_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19172\\3262174671.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_grouped = train_stage_1.groupby('user_id').apply(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19172\\3262174671.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  valid_grouped = valid_stage_1.groupby('user_id').apply(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_19172\\3262174671.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_grouped = test_df.groupby('user_id').apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>train_interactions</th>\n",
       "      <th>valid_interactions</th>\n",
       "      <th>test_interactions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>[(2657, 2021-06-03, 3), (3734, 2021-07-22, 3),...</td>\n",
       "      <td>[(12192, 2021-08-11, 2), (15719, 2021-08-11, 2...</td>\n",
       "      <td>[(142, 2021-08-13, 4), (10240, 2021-08-16, 0),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17</td>\n",
       "      <td>[(3734, 2021-08-01, 6)]</td>\n",
       "      <td>[(8618, 2021-08-06, 4), (10196, 2021-08-06, 4)]</td>\n",
       "      <td>[(9342, 2021-08-15, 6)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>[(4740, 2021-06-09, 2), (676, 2021-06-12, 5), ...</td>\n",
       "      <td>[(16484, 2021-08-11, 2), (3031, 2021-08-11, 2)]</td>\n",
       "      <td>[(15363, 2021-08-13, 4), (11253, 2021-08-13, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>[(13865, 2021-07-29, 3), (13361, 2021-07-29, 3...</td>\n",
       "      <td>[(12681, 2021-08-09, 0), (12846, 2021-08-11, 2)]</td>\n",
       "      <td>[(14488, 2021-08-20, 4)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53</td>\n",
       "      <td>[(13865, 2021-07-01, 3), (7571, 2021-07-01, 3)...</td>\n",
       "      <td>[(12250, 2021-08-10, 1)]</td>\n",
       "      <td>[(16426, 2021-08-15, 6), (15810, 2021-08-17, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                 train_interactions  \\\n",
       "0        3  [(2657, 2021-06-03, 3), (3734, 2021-07-22, 3),...   \n",
       "1       17                            [(3734, 2021-08-01, 6)]   \n",
       "2       30  [(4740, 2021-06-09, 2), (676, 2021-06-12, 5), ...   \n",
       "3       47  [(13865, 2021-07-29, 3), (13361, 2021-07-29, 3...   \n",
       "4       53  [(13865, 2021-07-01, 3), (7571, 2021-07-01, 3)...   \n",
       "\n",
       "                                  valid_interactions  \\\n",
       "0  [(12192, 2021-08-11, 2), (15719, 2021-08-11, 2...   \n",
       "1    [(8618, 2021-08-06, 4), (10196, 2021-08-06, 4)]   \n",
       "2    [(16484, 2021-08-11, 2), (3031, 2021-08-11, 2)]   \n",
       "3   [(12681, 2021-08-09, 0), (12846, 2021-08-11, 2)]   \n",
       "4                           [(12250, 2021-08-10, 1)]   \n",
       "\n",
       "                                   test_interactions  \n",
       "0  [(142, 2021-08-13, 4), (10240, 2021-08-16, 0),...  \n",
       "1                            [(9342, 2021-08-15, 6)]  \n",
       "2  [(15363, 2021-08-13, 4), (11253, 2021-08-13, 4...  \n",
       "3                           [(14488, 2021-08-20, 4)]  \n",
       "4  [(16426, 2021-08-15, 6), (15810, 2021-08-17, 1...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_grouped = train_stage_1.groupby('user_id').apply(\n",
    "    lambda x: [(t1, t2, t3) for t1, t2, t3 in sorted(zip(x.item_id, \n",
    "                                                 x.last_watch_dt,\n",
    "                                                 x.weekday), key=lambda x: x[1])]\n",
    ").reset_index()\n",
    "train_grouped.rename({0:'train_interactions'}, axis=1, inplace=True)\n",
    "\n",
    "valid_grouped = valid_stage_1.groupby('user_id').apply(\n",
    "    lambda x: [(t1, t2, t3) for t1, t2, t3 in sorted(zip(x.item_id,\n",
    "                                                         x.last_watch_dt,\n",
    "                                                         x.weekday), key=lambda x: x[1])]\n",
    ").reset_index()\n",
    "valid_grouped.rename({0:'valid_interactions'}, axis=1, inplace=True)\n",
    "\n",
    "test_grouped = test_df.groupby('user_id').apply(\n",
    "    lambda x: [(t1, t2, t3) for t1, t2, t3 in sorted(zip(x.item_id,\n",
    "                                                         x.last_watch_dt,\n",
    "                                                         x.weekday), key=lambda x: x[1])]\n",
    ").reset_index()\n",
    "test_grouped.rename({0:'test_interactions'}, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "joined = train_grouped.merge(valid_grouped).merge(test_grouped)\n",
    "joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Эвристическая модель (Popularity-based)\n",
    "class TopPopular:\n",
    "    def __init__(self):\n",
    "        self.trained = False\n",
    "    \n",
    "    def fit(self, df, col='train_interactions'):\n",
    "        counts = {}\n",
    "        for _, row in df.iterrows():\n",
    "            for item, _, _ in row[col]:\n",
    "                counts[item] = counts.get(item, 0) + 1\n",
    "                    \n",
    "        self.recommendations = [x[0] for x in sorted(counts.items(), \n",
    "                                                   key=lambda x: -x[1])]\n",
    "        self.trained = True\n",
    "        \n",
    "    def predict(self, df, topn=10) -> List[np.ndarray]:\n",
    "        assert self.trained\n",
    "        return [self.recommendations[:topn]] * len(df)\n",
    "    \n",
    "    def predict_score(self, user_id, item_id):\n",
    "        try:\n",
    "            rank = self.recommendations.index(item_id)\n",
    "            return 1.0 / (rank + 1)\n",
    "        except ValueError:\n",
    "            return 0.5\n",
    "# Инициализация и обучение моделей\n",
    "my_heuristic_model = TopPopular()\n",
    "my_heuristic_model.fit(joined)\n",
    "joined['toppopular_recs'] = my_heuristic_model.predict(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Матричная факторизация (SVD)\n",
    "class SVDRecommender:\n",
    "    def __init__(self, n_factors=50, n_epochs=20):\n",
    "        self.model = None\n",
    "        self.user_map = {}\n",
    "        self.item_map = {}\n",
    "        self.default_score = 0.5\n",
    "        self.trained = False\n",
    "        \n",
    "    def fit(self, df, col='train_interactions'):\n",
    "        # Create mappings\n",
    "        users = set()\n",
    "        items = set()\n",
    "        interactions = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            users.add(user_id)\n",
    "            for item_id, _, rating in row[col]:\n",
    "                items.add(item_id)\n",
    "                interactions.append((user_id, item_id, rating))\n",
    "                \n",
    "        self.user_map = {u: i for i, u in enumerate(users)}\n",
    "        self.item_map = {i: j for j, i in enumerate(items)}\n",
    "        \n",
    "        # Prepare data for surprise\n",
    "        reader = Reader(rating_scale=(0, 1))\n",
    "        data = Dataset.load_from_df(pd.DataFrame(interactions, \n",
    "                                               columns=['user_id', 'item_id', 'rating']), \n",
    "                                  reader)\n",
    "        trainset = data.build_full_trainset()\n",
    "        \n",
    "        # Train model\n",
    "        self.model = SVD(n_factors=n_factors, n_epochs=n_epochs, verbose=False)\n",
    "        self.model.fit(trainset)\n",
    "        self.trained = True\n",
    "        \n",
    "    def predict(self, df, topn=10) -> List[np.ndarray]:\n",
    "        assert self.trained\n",
    "        all_items = list(self.item_map.keys())\n",
    "        recommendations = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            scores = []\n",
    "            for item_id in all_items:\n",
    "                try:\n",
    "                    score = self.model.predict(self.user_map[user_id], \n",
    "                                            self.item_map[item_id]).est\n",
    "                    scores.append((item_id, score))\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            top_items = [x[0] for x in sorted(scores, key=lambda x: -x[1])[:topn]]\n",
    "            recommendations.append(top_items)\n",
    "            \n",
    "        return recommendations\n",
    "    \n",
    "    def predict_score(self, user_id, item_id):\n",
    "        try:\n",
    "            return self.model.predict(self.user_map[user_id], \n",
    "                                   self.item_map[item_id]).est\n",
    "        except:\n",
    "            return self.default_score\n",
    "\n",
    "my_matrix_factorization = SVDRecommender()\n",
    "my_matrix_factorization.fit(joined)\n",
    "joined['svd_recs'] = my_matrix_factorization.predict(joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Эвристическая модель (Popularity-based)\n",
    "class PopularityRecommender:\n",
    "    def __init__(self):\n",
    "        self.popular_items = None\n",
    "        \n",
    "    def fit(self, df):\n",
    "        # Ранжируем товары по количеству взаимодействий\n",
    "        self.popular_items = df['item_id'].value_counts().reset_index()\n",
    "        self.popular_items.columns = ['item_id', 'score']\n",
    "        \n",
    "    def recommend(self, user_id, n=10):\n",
    "        return self.popular_items.head(n)\n",
    "\n",
    "popular_model = PopularityRecommender()\n",
    "popular_model.fit(train_stage_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DZ_RECSYS\\.venv\\lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed csc_matrix instead. Converting to CSR took 0.08099079132080078 seconds\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:37<00:00,  2.49s/it]\n"
     ]
    }
   ],
   "source": [
    "# 2. Матричная факторизация (implicit ALS)\n",
    "def prepare_implicit_data(df):\n",
    "    user_ids = df['user_id'].astype('category').cat.codes\n",
    "    item_ids = df['item_id'].astype('category').cat.codes\n",
    "    interactions = csr_matrix((np.ones(len(df)), (user_ids, item_ids)))\n",
    "    user_map = dict(enumerate(df['user_id'].astype('category').cat.categories))\n",
    "    item_map = dict(enumerate(df['item_id'].astype('category').cat.categories))\n",
    "    return interactions, user_map, item_map\n",
    "\n",
    "interactions, user_map, item_map = prepare_implicit_data(train_stage_1)\n",
    "\n",
    "als_model = implicit.als.AlternatingLeastSquares(\n",
    "    factors=64, \n",
    "    iterations=15, \n",
    "    regularization=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "als_model.fit(interactions.T)\n",
    "\n",
    "def als_predict(user_id, n=10):\n",
    "    if user_id not in user_map.values():\n",
    "        return pd.DataFrame({'item_id': [], 'als_score': []})\n",
    "    internal_id = list(user_map.keys())[list(user_map.values()).index(user_id)]\n",
    "    items, scores = als_model.recommend(internal_id, interactions[internal_id], N=n)\n",
    "    return pd.DataFrame({\n",
    "        'item_id': [item_map[i] for i in items],\n",
    "        'als_score': scores\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'weekday'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19172\\4232430618.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 3. Нейросетевая модель (Neural Collaborative Filtering)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m train_grouped = train_stage_1.groupby('user_id').apply(\n\u001b[0m\u001b[0;32m      3\u001b[0m     lambda x: [(t1, t2, t3) for t1, t2, t3 in sorted(zip(x.item_id, \n\u001b[0;32m      4\u001b[0m                                                  \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_watch_dt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                  x.weekday), key=lambda x: x[1])]\n",
      "\u001b[1;32md:\\DZ_RECSYS\\.venv\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1833\u001b[0m                         ),\n\u001b[0;32m   1834\u001b[0m                         \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1835\u001b[0m                         \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1836\u001b[0m                     )\n\u001b[1;32m-> 1837\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1838\u001b[0m                 \u001b[1;31m# gh-20949\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1839\u001b[0m                 \u001b[1;31m# try again, with .apply acting as a filtering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1840\u001b[0m                 \u001b[1;31m# operation, by excluding the grouping column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\DZ_RECSYS\\.venv\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[0;32m   1881\u001b[0m         \u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mSeries\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1883\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1884\u001b[0m         \"\"\"\n\u001b[1;32m-> 1885\u001b[1;33m         \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_grouper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_groupwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1886\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnot_indexed_same\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1887\u001b[0m             \u001b[0mnot_indexed_same\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmutated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1888\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\DZ_RECSYS\\.venv\\lib\\site-packages\\pandas\\core\\groupby\\ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, f, data, axis)\u001b[0m\n\u001b[0;32m    915\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m             \u001b[1;31m# group might be modified\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 919\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    920\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmutated\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m             \u001b[0mresult_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19172\\4232430618.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      3\u001b[0m     lambda x: [(t1, t2, t3) for t1, t2, t3 in sorted(zip(x.item_id, \n\u001b[0;32m      4\u001b[0m                                                  \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_watch_dt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                                                  x.weekday), key=lambda x: x[1])]\n\u001b[0m",
      "\u001b[1;32md:\\DZ_RECSYS\\.venv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         ):\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'weekday'"
     ]
    }
   ],
   "source": [
    "# 3. Нейросетевая модель (Neural Collaborative Filtering)\n",
    "train_grouped = train_stage_1.groupby('user_id').apply(\n",
    "    lambda x: [(t1, t2, t3) for t1, t2, t3 in sorted(zip(x.item_id, \n",
    "                                                 x.last_watch_dt,\n",
    "                                                 x.weekday), key=lambda x: x[1])]\n",
    ").reset_index()\n",
    "train_grouped.rename({0:'train_interactions'}, axis=1, inplace=True)\n",
    "\n",
    "valid_grouped = valid_stage_1.groupby('user_id').apply(\n",
    "    lambda x: [(t1, t2, t3) for t1, t2, t3 in sorted(zip(x.item_id,\n",
    "                                                         x.last_watch_dt,\n",
    "                                                         x.weekday), key=lambda x: x[1])]\n",
    ").reset_index()\n",
    "valid_grouped.rename({0:'valid_interactions'}, axis=1, inplace=True)\n",
    "\n",
    "test_grouped = test_df.groupby('user_id').apply(\n",
    "    lambda x: [(t1, t2, t3) for t1, t2, t3 in sorted(zip(x.item_id,\n",
    "                                                         x.last_watch_dt,\n",
    "                                                         x.weekday), key=lambda x: x[1])]\n",
    ").reset_index()\n",
    "test_grouped.rename({0:'test_interactions'}, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "train_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация признаков для второго уровня\n",
    "def generate_features(df):\n",
    "    results = []\n",
    "    \n",
    "    for user_id in df['user_id'].unique():\n",
    "        # Получаем рекомендации от всех моделей\n",
    "        pop_recs = popular_model.predict(user_id)\n",
    "        als_recs = als_predict(user_id)\n",
    "        ncf_recs = ncf_predict(user_id)\n",
    "        \n",
    "        # Объединяем результаты\n",
    "        merged = pop_recs.merge(als_recs, on='item_id', how='outer')\\\n",
    "                        .merge(ncf_recs, on='item_id', how='outer')\n",
    "        merged['user_id'] = user_id\n",
    "        results.append(merged)\n",
    "    \n",
    "    return pd.concat(results).fillna(0)\n",
    "\n",
    "train_features = generate_features(train_stage_1)\n",
    "valid_features = generate_features(valid_stage_1)\n",
    "\n",
    "# Сохранение признаков\n",
    "train_features.to_csv('train_features.csv', index=False)\n",
    "valid_features.to_csv('valid_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3627771860.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    my_heuristic_model = # YOUR CODE HERE\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "my_heuristic_model = # YOUR CODE HERE\n",
    "my_matrix_factorization = # YOUR CODE HERE\n",
    "my_neural_network = # YOUR CODE HERE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждая модель должна уметь:\n",
    "1) для пары user_item предсказывать скор релевантности (масштаб скора не важен), важно обработать случаи, когда модель не можеn проскорить пользователя или айтем, вместо этого вернуть какое-то дефолтное значение\n",
    "2) для всех пользователей вернуть top-k самых релевантных айтемов (тут вам скоры не нужны)\n",
    "\n",
    "\n",
    "Дополнительно можно провести анализ кандидат генератов, измерить насколько различные айтемы они рекомендуют, например с помощью таких метрик как: [Ranked based overlap](https://github.com/changyaochen/rbo) или различные вариации [Diversity](https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/master/Base/Evaluation/metrics.py#L289). **(1 балл)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2 Этап. Генерация и сборка признаков. (max 2 балла)\n",
    "Необходимо собрать минимум 10 осмысленных (`np.radndom.rand()` не подойдет) признаков, при этом:\n",
    "1. 2 должны относиться только к сущности \"пользователь\" (например средний % просмотра фильмов у этой возрастной категории)\n",
    "2. 2 должны относиться только к сущности \"айтем\" (например средний средний % просмотра данного фильма)\n",
    "3. 6 признаков, которые показывают связь пользователя и айтема (например средний % просмотра фильмов с данным актером (айтем) у пользователей с таким же полом (пользователь)). \n",
    "\n",
    "### ВАЖНО!  \n",
    "\n",
    "1. **В датасете есть колонка `watched_prct`. Ее можно использовать для генерации признаков (например сколько пользователь в среднем смотрит фильмы), но нельзя подавать в модель, как отдельную фичу, потому что она напрямую связана с target.**\n",
    "2. **Все признаки должны быть собраны без дата лика, то есть если пользователь посмотрел фильм 10 августа, то признаки мы можем считать только на данных до 9 августа включительно.**\n",
    "\n",
    "\n",
    "### Разбалловка\n",
    "Обучение ранкера будет проходить на `valid_stage_1`, как  раз на которой мы валидировали модели, а тестировать на `test`. Поэтому есть 2 варианта сборки признаков, **реализовать нужно только 1 из них:**\n",
    "1. Для обучения собираем признаки на первый день `valid_stage_1`, а для теста на первый день `test`. Например, если `valid_stage_1` начинается 5 сентября, то все признаки мы можем собирать только по 4 сентября включительно. **(1 балл)**\n",
    "2. Признаки будем собирать честно на каждый день, то есть на 5 сентября собираем с начала до 4, на 6 сентября с начала до 5 и т.д. **(2 балла)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_with_features = # YOUR CODE IS HERE\n",
    "test_df_with_features = # YOUR CODE IS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3 Этап. Обучение финального ранкера (max 2 балла)\n",
    "Собрав все признаки из этапа 2, добавив скоры моделей из этапа 1 для каждой пары пользователь-айтем (где это возможно), пришло время обучать ранкер. В качестве ранкера можно использовать либо [xgboost](https://xgboost.readthedocs.io/en/stable/) или [catboost](https://catboost.ai/). Обучать можно как `Classfier`, так и `Ranker`, выбираем то, что лучше сработает. Обучение ранкера будет проходить на `valid_stage_1`, как  раз на которой мы валидировали модели, а тестировать на `test`, которую мы до сих пор не трогали.  Заметьте, что у нас в тесте есть холодные пользователи – те, кого не было в train и активные – те, кто был в train. Возможно их стоит обработать по отдельности (а может и нет).  \n",
    "(1 балл)\n",
    "\n",
    "После получения лучшей модели надо посмотреть на важность признаков и [shap values](https://shap.readthedocs.io/en/latest/index.html), чтобы:\n",
    "1. Интерпритировать признаки, которые вы собрали, насколько они полезные\n",
    "2. Проверить наличие ликов – если важность фичи в 100 раз больше, чем у всех остальных, то явно что-то не то  \n",
    "\n",
    "(1 балл)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR FIT PREDICT CODE HERE\n",
    "model.fit()\n",
    "model.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4 Этап. Инференс лучшего ранкера (max 3 балла)\n",
    "\n",
    "Теперь мы хотим построить рекомендации \"на завтра\", для этого нам нужно:\n",
    "\n",
    "1. Обучить модели первого уровня на всех (train+test) данных (0.5 балла)\n",
    "2. Для каждой модели первого уровня для каждого пользователя сгененировать N кандидатов (0.5 балла)\n",
    "3. \"Склеить\" всех кандидатов для каждого пользователя (дубли выкинуть), посчитать скоры от всех моделей (0.5 балла)\n",
    "4. Собрать фичи для ваших кандидатов (теперь можем считать признаки на всех данных) (0.5 балла)\n",
    "5. Проскорить всех кандидатов бустингом и оставить k лучших (0.5 балла)\n",
    "6. Посчитать разнообразие(Diversity) и построить график от Diversity(k) (0.5 балла)\n",
    "\n",
    "\n",
    "Все гиперпараметры (N, k) определяете только Вы!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
