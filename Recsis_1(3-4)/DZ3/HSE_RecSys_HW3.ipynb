{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ №3 Двухуровневый пайплайн\n",
    "#### В этой домашке вам предстоит написать с нуля двустадийную рекомендательную систему. \n",
    "\n",
    "#### Дата выдачи: 10.03.25\n",
    "\n",
    "#### Мягкий дедлайн: 31.03.25 23:59 MSK\n",
    "\n",
    "#### Жесткий дедлайн: 7.04.25 23:59 MSK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание\n",
    "Это творческое задание, в котором вам необходимо реализовать полный цикл построения рекомендательной системы: реализовать кандидат генераторов, придумать и собрать признаки, обучить итоговый ранкер и заинференсить модели на всех пользователей.\n",
    "\n",
    "Вам предоставляется два набора данных: `train.csv` и `test.csv` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import implicit\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from gdown) (4.13.3)\n",
      "Requirement already satisfied: filelock in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from gdown) (3.18.0)\n",
      "Requirement already satisfied: requests[socks] in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from gdown) (4.67.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from beautifulsoup4->gdown) (4.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from requests[socks]->gdown) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from requests[socks]->gdown) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from requests[socks]->gdown) (2025.1.31)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: colorama in d:\\hse\\recsis_1(3-4)\\dz3\\.venv\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HSE\\Recsis_1(3-4)\\DZ3\\.venv\\lib\\site-packages\\gdown\\parse_url.py:48: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=1-CcS22-UpTJeNcFlA0dVLrEQn8jnI0d-\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/file/d/1-CcS22-UpTJeNcFlA0dVLrEQn8jnI0d-/view?usp=drive_link\n",
      "To: d:\\HSE\\Recsis_1(3-4)\\DZ3\\train.csv\n",
      "91.5kB [00:00, 3.23MB/s]\n",
      "d:\\HSE\\Recsis_1(3-4)\\DZ3\\.venv\\lib\\site-packages\\gdown\\parse_url.py:48: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=11iz3xDh0IIoEIBY0dyRSvByY3qfiT3BG\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/file/d/11iz3xDh0IIoEIBY0dyRSvByY3qfiT3BG/view?usp=drive_link\n",
      "To: d:\\HSE\\Recsis_1(3-4)\\DZ3\\test.csv\n",
      "91.6kB [00:00, 3.17MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# скачиваем данные\n",
    "# если из этой ячейки не получается, то вот ссылка на папку https://drive.google.com/drive/folders/1HT0Apm8Jft0VPLJtdBBUGu9s1M7vZcoJ?usp=drive_link\n",
    "\n",
    "!pip3 install gdown\n",
    "\n",
    "\n",
    "import gdown\n",
    "# train\n",
    "url = \"https://drive.google.com/file/d/1-CcS22-UpTJeNcFlA0dVLrEQn8jnI0d-/view?usp=drive_link\"\n",
    "output = 'train.csv'\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "# test\n",
    "url = \"https://drive.google.com/file/d/11iz3xDh0IIoEIBY0dyRSvByY3qfiT3BG/view?usp=drive_link\"\n",
    "output = 'test.csv'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1 Этап. Модели первого уровня. (max 3 балла)\n",
    "В этом этапе вам необходимо разделить `train` датасет на 2 части: для обучения моделей первого уровня и для их валидации. Единственное условие для разбиения – разбивать нужно по времени. Данные для обучение будем называть `train_stage_1`, данные для валидации `valid_stage_1`. Объемы этих датасетов вы определяет самостоятельно. \n",
    "\n",
    "Для начала нам нужно отобрать кандидатов при помощи легких моделей. Необходимо реализовать 3 типа моделей:\n",
    "1. Любая эвристическая(алгоритмичная) модель на ваш выбор **(0.5 балл)**\n",
    "2. Любая матричная факторизация на ваш выбор **(1 балл)**\n",
    "3. Любая нейросетевая модель на ваш выбор **(1 балла)**\n",
    "\n",
    "Не забудьте использовать скор каждой модели, как признак!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-08-12'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка данных\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "train_df.last_watch_dt.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование временных меток\n",
    "train_df['weekday'] = pd.to_datetime(train_df.last_watch_dt).dt.weekday\n",
    "test_df['weekday'] = pd.to_datetime(test_df.last_watch_dt).dt.weekday\n",
    "train_df = train_df.sort_values('last_watch_dt')\n",
    "\n",
    "train_stage_1 = train_df.loc[(train_df.last_watch_dt < '2021-08-06')].copy()\n",
    "valid_stage_1 = train_df.loc[train_df.last_watch_dt >= '2021-08-06'].copy()\n",
    "\n",
    "user_ids = train_df['user_id'].unique()\n",
    "item_ids = train_df['item_id'].unique()\n",
    "user_to_idx = {user: idx for idx, user in enumerate(user_ids)}\n",
    "idx_to_user = {idx: user for user, idx in user_to_idx.items()}\n",
    "item_to_idx = {item: idx for idx, item in enumerate(item_ids)}\n",
    "idx_to_item = {idx: item for item, idx in item_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((608467, 7), (4473822, 7), (393134, 7))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape, train_stage_1.shape, valid_stage_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Эвристическая модель (популярность по дням недели)\n",
    "class HeuristicModel:\n",
    "    def __init__(self):\n",
    "        self.popularity = defaultdict(dict)\n",
    "        self.default_score = 0.5\n",
    "        \n",
    "    def fit(self, df):\n",
    "        for weekday in range(7):\n",
    "            weekday_data = df[df['weekday'] == weekday]\n",
    "            item_counts = weekday_data['item_id'].value_counts()\n",
    "            total = item_counts.sum()\n",
    "            for item, count in item_counts.items():\n",
    "                self.popularity[weekday][item] = count / total\n",
    "    \n",
    "    def predict(self, user_id, item_id, weekday=None, default=None):\n",
    "        if weekday is None:\n",
    "            weekday = np.random.randint(0, 7)\n",
    "            \n",
    "        if item_id in self.popularity[weekday]:\n",
    "            return self.popularity[weekday][item_id]\n",
    "        return default if default is not None else self.default_score\n",
    "    \n",
    "    def recommend(self, user_id, k=10, weekday=None):\n",
    "        if weekday is None:\n",
    "            weekday = np.random.randint(0, 7)\n",
    "            \n",
    "        popular_items = sorted(self.popularity[weekday].items(), \n",
    "                             key=lambda x: -x[1])\n",
    "        return [item for item, score in popular_items[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Матричная факторизация (implicit ALS)\n",
    "class ImplicitFactorizationModel:\n",
    "    def __init__(self, factors=50, iterations=15, regularization=0.01):\n",
    "        self.model = implicit.als.AlternatingLeastSquares(\n",
    "            factors=factors,\n",
    "            iterations=iterations,\n",
    "            regularization=regularization,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.default_score = 0.5\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        \n",
    "    def fit(self, df):\n",
    "        # Создаем разреженную матрицу взаимодействий\n",
    "        user_indices = df['user_id'].map(user_to_idx).values\n",
    "        item_indices = df['item_id'].map(item_to_idx).values\n",
    "        durations = df['total_dur'].values\n",
    "        \n",
    "        # Нормализуем продолжительности\n",
    "        durations = durations / durations.max()\n",
    "        \n",
    "        # Создаем матрицу в формате COO\n",
    "        from scipy.sparse import coo_matrix\n",
    "        interactions = coo_matrix(\n",
    "            (durations, (user_indices, item_indices)),\n",
    "            shape=(len(user_to_idx), len(item_to_idx))\n",
    "        )\n",
    "        \n",
    "        # Обучаем модель\n",
    "        self.model.fit(interactions)\n",
    "        self.user_factors = self.model.user_factors\n",
    "        self.item_factors = self.model.item_factors\n",
    "        \n",
    "    def predict(self, user_id, item_id, default=None):\n",
    "        if user_id not in user_to_idx or item_id not in item_to_idx:\n",
    "            return default if default is not None else self.default_score\n",
    "            \n",
    "        user_idx = user_to_idx[user_id]\n",
    "        item_idx = item_to_idx[item_id]\n",
    "        return self.user_factors[user_idx] @ self.item_factors[item_idx].T\n",
    "    \n",
    "    def recommend(self, user_id, k=10):\n",
    "        if user_id not in user_to_idx:\n",
    "            return []\n",
    "            \n",
    "        user_idx = user_to_idx[user_id]\n",
    "        items, scores = self.model.recommend(\n",
    "            user_idx, \n",
    "            user_items=None, \n",
    "            N=k, \n",
    "            filter_already_liked_items=False\n",
    "        )\n",
    "        return [idx_to_item[item] for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Нейросетевая модель (PyTorch)\n",
    "class InteractionDataset(Dataset):\n",
    "    def __init__(self, df, user_to_idx, item_to_idx):\n",
    "        self.df = df.copy()\n",
    "        self.df['user_idx'] = self.df['user_id'].map(user_to_idx)\n",
    "        self.df['item_idx'] = self.df['item_id'].map(item_to_idx)\n",
    "        self.df = self.df.dropna(subset=['user_idx', 'item_idx'])\n",
    "        \n",
    "        # Нормализуем target\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.df['target'] = self.scaler.fit_transform(\n",
    "            self.df[['total_dur']]\n",
    "        )\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        return (\n",
    "            torch.LongTensor([row['user_idx']]),\n",
    "            torch.LongTensor([row['item_idx']]),\n",
    "            torch.FloatTensor([row['target']])\n",
    "        )\n",
    "\n",
    "class NeuralCF(nn.Module):\n",
    "    def __init__(self, n_users, n_items, emb_dim=64):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, emb_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, emb_dim)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, user, item):\n",
    "        user_emb = self.user_embedding(user).squeeze(1)\n",
    "        item_emb = self.item_embedding(item).squeeze(1)\n",
    "        x = torch.cat([user_emb, item_emb], dim=1)\n",
    "        return self.fc(x)\n",
    "\n",
    "class TorchNeuralModel:\n",
    "    def __init__(self, emb_dim=64, lr=0.001, batch_size=256, n_epochs=5):\n",
    "        self.model = None\n",
    "        self.emb_dim = emb_dim\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.default_score = 0.5\n",
    "        self.scaler = MinMaxScaler()\n",
    "        \n",
    "    def fit(self, df):\n",
    "        # Подготовка данных\n",
    "        dataset = InteractionDataset(df, user_to_idx, item_to_idx)\n",
    "        self.scaler = dataset.scaler\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        # Инициализация модели\n",
    "        n_users = len(user_to_idx)\n",
    "        n_items = len(item_to_idx)\n",
    "        self.model = NeuralCF(n_users, n_items, self.emb_dim)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Обучение\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for user, item, target in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                output = self.model(user, item)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    def predict(self, user_id, item_id, default=None):\n",
    "        if user_id not in user_to_idx or item_id not in item_to_idx:\n",
    "            return default if default is not None else self.default_score\n",
    "            \n",
    "        user_idx = torch.LongTensor([user_to_idx[user_id]])\n",
    "        item_idx = torch.LongTensor([item_to_idx[item_id]])\n",
    "        with torch.no_grad():\n",
    "            score = self.model(user_idx, item_idx).item()\n",
    "        return score\n",
    "    \n",
    "    def recommend(self, user_id, k=10):\n",
    "        if user_id not in user_to_idx:\n",
    "            return []\n",
    "            \n",
    "        user_idx = user_to_idx[user_id]\n",
    "        user_tensor = torch.LongTensor([user_idx] * len(item_to_idx))\n",
    "        item_tensor = torch.LongTensor(list(item_to_idx.values()))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            scores = self.model(user_tensor, item_tensor).flatten().numpy()\n",
    "        \n",
    "        top_items = np.argsort(-scores)[:k]\n",
    "        return [idx_to_item[item] for item in top_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Heuristic Model...\n"
     ]
    }
   ],
   "source": [
    "# Инициализация и обучение моделей\n",
    "print(\"Training Heuristic Model...\")\n",
    "heuristic_model = HeuristicModel()\n",
    "heuristic_model.fit(train_stage_1)\n",
    "print(\"Training Implicit ALS Model...\")\n",
    "implicit_model = ImplicitFactorizationModel(factors=50, iterations=15)\n",
    "implicit_model.fit(train_stage_1)\n",
    "print(\"Training PyTorch Neural Model...\")\n",
    "torch_model = TorchNeuralModel(emb_dim=64, n_epochs=5)\n",
    "torch_model.fit(train_stage_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Implicit ALS Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HSE\\Recsis_1(3-4)\\DZ3\\.venv\\lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.3573026657104492 seconds\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:57<00:00,  3.81s/it]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Implicit ALS Model...\")\n",
    "implicit_model = ImplicitFactorizationModel(factors=50, iterations=15)\n",
    "implicit_model.fit(train_stage_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PyTorch Neural Model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining PyTorch Neural Model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m torch_model \u001b[38;5;241m=\u001b[39m TorchNeuralModel(emb_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtorch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_stage_1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 76\u001b[0m, in \u001b[0;36mTorchNeuralModel.fit\u001b[1;34m(self, df)\u001b[0m\n\u001b[0;32m     74\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[0;32m     75\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 76\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\HSE\\Recsis_1(3-4)\\DZ3\\.venv\\lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32md:\\HSE\\Recsis_1(3-4)\\DZ3\\.venv\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32md:\\HSE\\Recsis_1(3-4)\\DZ3\\.venv\\lib\\site-packages\\torch\\optim\\adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    235\u001b[0m         group,\n\u001b[0;32m    236\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    241\u001b[0m         state_steps,\n\u001b[0;32m    242\u001b[0m     )\n\u001b[1;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32md:\\HSE\\Recsis_1(3-4)\\DZ3\\.venv\\lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\HSE\\Recsis_1(3-4)\\DZ3\\.venv\\lib\\site-packages\\torch\\optim\\adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\HSE\\Recsis_1(3-4)\\DZ3\\.venv\\lib\\site-packages\\torch\\optim\\adam.py:425\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    423\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m device_beta1)\n\u001b[1;32m--> 425\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[0;32m    428\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training PyTorch Neural Model...\")\n",
    "torch_model = TorchNeuralModel(emb_dim=64, n_epochs=5)\n",
    "torch_model.fit(train_stage_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для оценки разнообразия рекомендаций\n",
    "def evaluate_diversity(models, users, k=10):\n",
    "    overlaps = np.zeros((len(models), len(models)))\n",
    "    diversities = []\n",
    "    \n",
    "    for user in users[:1000]:  # Ограничим выборку для скорости\n",
    "        recommendations = []\n",
    "        for model in models:\n",
    "            try:\n",
    "                recs = set(model.recommend(user, k=k))\n",
    "                recommendations.append(recs)\n",
    "            except:\n",
    "                recommendations.append(set())\n",
    "        \n",
    "        # Рассчитываем попарные пересечения\n",
    "        for i in range(len(models)):\n",
    "            for j in range(len(models)):\n",
    "                overlaps[i,j] += len(recommendations[i] & recommendations[j]) / k\n",
    "                \n",
    "        # Рассчитываем diversity для одного пользователя\n",
    "        all_recs = set()\n",
    "        for rec in recommendations:\n",
    "            all_recs.update(rec)\n",
    "        diversity = len(all_recs) / (len(models) * k)\n",
    "        diversities.append(diversity)\n",
    "    \n",
    "    overlaps /= min(1000, len(users))\n",
    "    mean_diversity = np.mean(diversities)\n",
    "    \n",
    "    print(\"Pairwise Overlap Matrix:\")\n",
    "    print(overlaps)\n",
    "    print(f\"\\nMean Diversity: {mean_diversity:.3f}\")\n",
    "    \n",
    "    return overlaps, mean_diversity\n",
    "\n",
    "# Оценка разнообразия рекомендаций\n",
    "print(\"\\nEvaluating Recommendation Diversity...\")\n",
    "models = [heuristic_model, implicit_model, torch_model]\n",
    "sample_users = valid_stage_1['user_id'].unique()[:1000]\n",
    "overlaps, diversity = evaluate_diversity(models, sample_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждая модель должна уметь:\n",
    "1) для пары user_item предсказывать скор релевантности (масштаб скора не важен), важно обработать случаи, когда модель не можеn проскорить пользователя или айтем, вместо этого вернуть какое-то дефолтное значение\n",
    "2) для всех пользователей вернуть top-k самых релевантных айтемов (тут вам скоры не нужны)\n",
    "\n",
    "\n",
    "Дополнительно можно провести анализ кандидат генератов, измерить насколько различные айтемы они рекомендуют, например с помощью таких метрик как: [Ranked based overlap](https://github.com/changyaochen/rbo) или различные вариации [Diversity](https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation/blob/master/Base/Evaluation/metrics.py#L289). **(1 балл)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2 Этап. Генерация и сборка признаков. (max 2 балла)\n",
    "Необходимо собрать минимум 10 осмысленных (`np.radndom.rand()` не подойдет) признаков, при этом:\n",
    "1. 2 должны относиться только к сущности \"пользователь\" (например средний % просмотра фильмов у этой возрастной категории)\n",
    "2. 2 должны относиться только к сущности \"айтем\" (например средний средний % просмотра данного фильма)\n",
    "3. 6 признаков, которые показывают связь пользователя и айтема (например средний % просмотра фильмов с данным актером (айтем) у пользователей с таким же полом (пользователь)). \n",
    "\n",
    "### ВАЖНО!  \n",
    "\n",
    "1. **В датасете есть колонка `watched_prct`. Ее можно использовать для генерации признаков (например сколько пользователь в среднем смотрит фильмы), но нельзя подавать в модель, как отдельную фичу, потому что она напрямую связана с target.**\n",
    "2. **Все признаки должны быть собраны без дата лика, то есть если пользователь посмотрел фильм 10 августа, то признаки мы можем считать только на данных до 9 августа включительно.**\n",
    "\n",
    "\n",
    "### Разбалловка\n",
    "Обучение ранкера будет проходить на `valid_stage_1`, как  раз на которой мы валидировали модели, а тестировать на `test`. Поэтому есть 2 варианта сборки признаков, **реализовать нужно только 1 из них:**\n",
    "1. Для обучения собираем признаки на первый день `valid_stage_1`, а для теста на первый день `test`. Например, если `valid_stage_1` начинается 5 сентября, то все признаки мы можем собирать только по 4 сентября включительно. **(1 балл)**\n",
    "2. Признаки будем собирать честно на каждый день, то есть на 5 сентября собираем с начала до 4, на 6 сентября с начала до 5 и т.д. **(2 балла)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_with_features = # YOUR CODE IS HERE\n",
    "test_df_with_features = # YOUR CODE IS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3 Этап. Обучение финального ранкера (max 2 балла)\n",
    "Собрав все признаки из этапа 2, добавив скоры моделей из этапа 1 для каждой пары пользователь-айтем (где это возможно), пришло время обучать ранкер. В качестве ранкера можно использовать либо [xgboost](https://xgboost.readthedocs.io/en/stable/) или [catboost](https://catboost.ai/). Обучать можно как `Classfier`, так и `Ranker`, выбираем то, что лучше сработает. Обучение ранкера будет проходить на `valid_stage_1`, как  раз на которой мы валидировали модели, а тестировать на `test`, которую мы до сих пор не трогали.  Заметьте, что у нас в тесте есть холодные пользователи – те, кого не было в train и активные – те, кто был в train. Возможно их стоит обработать по отдельности (а может и нет).  \n",
    "(1 балл)\n",
    "\n",
    "После получения лучшей модели надо посмотреть на важность признаков и [shap values](https://shap.readthedocs.io/en/latest/index.html), чтобы:\n",
    "1. Интерпритировать признаки, которые вы собрали, насколько они полезные\n",
    "2. Проверить наличие ликов – если важность фичи в 100 раз больше, чем у всех остальных, то явно что-то не то  \n",
    "\n",
    "(1 балл)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR FIT PREDICT CODE HERE\n",
    "model.fit()\n",
    "model.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4 Этап. Инференс лучшего ранкера (max 3 балла)\n",
    "\n",
    "Теперь мы хотим построить рекомендации \"на завтра\", для этого нам нужно:\n",
    "\n",
    "1. Обучить модели первого уровня на всех (train+test) данных (0.5 балла)\n",
    "2. Для каждой модели первого уровня для каждого пользователя сгененировать N кандидатов (0.5 балла)\n",
    "3. \"Склеить\" всех кандидатов для каждого пользователя (дубли выкинуть), посчитать скоры от всех моделей (0.5 балла)\n",
    "4. Собрать фичи для ваших кандидатов (теперь можем считать признаки на всех данных) (0.5 балла)\n",
    "5. Проскорить всех кандидатов бустингом и оставить k лучших (0.5 балла)\n",
    "6. Посчитать разнообразие(Diversity) и построить график от Diversity(k) (0.5 балла)\n",
    "\n",
    "\n",
    "Все гиперпараметры (N, k) определяете только Вы!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
