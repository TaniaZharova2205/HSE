{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W8R8WgZceEk"
   },
   "source": [
    "# Семинар 6: Character-Level LSTM\n",
    "\n",
    "## Вступление\n",
    "На прошлом занятии мы познакомились с тем, как можно векторизовать текстовые данные для решения задач обработки текстов. Сегодня мы продолжим заниматься текстами и посмотрим на простейший пример автоматической генерации текстов при помощи Recurrent Neural Network (RNN).\n",
    "\n",
    "Полезные материалы по RNN можно почитать [здесь](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), а реализацию на PyTorch — [здесь](https://github.com/karpathy/char-rnn).\n",
    "\n",
    "### План семинара\n",
    "1. Подготовка данных\n",
    "2. Имплементация модели\n",
    "3. Обучение модели\n",
    "4. Применение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OD5nh3nBPs0V",
    "outputId": "28a64d07-a536-4199-cc77-27a9bfa12c23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install lightning -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sqUOE2flceEl"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wHfCDyzceEl"
   },
   "source": [
    "## 1. Подготовка данных\n",
    "\n",
    "### Загрузим текст \"Анны Карениной\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0OdxVLREPXTa",
    "outputId": "279fc7be-0813-4591-8448-bfbd2e978445"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-17 15:12:05--  https://raw.githubusercontent.com/hse-ds/iad-deep-learning/master/2022/seminars/sem09/anna.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1985223 (1.9M) [text/plain]\n",
      "Saving to: ‘anna.txt’\n",
      "\n",
      "anna.txt            100%[===================>]   1.89M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2025-05-17 15:12:05 (43.8 MB/s) - ‘anna.txt’ saved [1985223/1985223]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/hse-ds/iad-deep-learning/master/2022/seminars/sem09/anna.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "b34kfqIOceEl",
    "outputId": "2f7ffbd1-5537-4847-fdd5-a10b6c539da2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"anna.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iC21bopceEl"
   },
   "source": [
    "### Токенизируем текст\n",
    "\n",
    "Аналогично предыдущему семинару, в ячейках ниже создадим два словаря для преобразования символов в целые числа и обратно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYVlmnxLceEl",
    "outputId": "a00fc186-31f1-4cf7-9812-dc1181af2d20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23, 49, 32, 43, 48, 74,  0, 77, 12, 33, 33, 33, 36, 32, 43, 43, 54, 77,\n",
       "        46, 32, 41, 55, 61, 55, 74, 22, 77, 32,  0, 74, 77, 32, 61, 61, 77, 32,\n",
       "        61, 55, 26, 74, 19, 77, 74, 20, 74,  0, 54, 77, 62,  8, 49, 32, 43, 43,\n",
       "        54, 77, 46, 32, 41, 55, 61, 54, 77, 55, 22, 77, 62,  8, 49, 32, 43, 43,\n",
       "        54, 77, 55,  8, 77, 55, 48, 22, 77, 24, 70,  8, 33, 70, 32, 54,  5, 33,\n",
       "        33,  3, 20, 74,  0, 54, 48, 49, 55,  8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_chars = tuple(set(text))\n",
    "int2char = dict(enumerate(unique_chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = torch.tensor([char2int[ch] for ch in text])\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azltQy-gceEl"
   },
   "source": [
    "Посмотрим на схему char-RNN:\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charseq.jpeg?raw=1\" width=\"30%\">\n",
    "\n",
    "Сеть ожидает **one-hot encoded** входа, что означает, что каждый символ преобразуется в целое число (через созданный маппинг), а затем преобразуется в вектор-столбец, где только соответствующий ему целочисленный индекс будет иметь значение 1, а остальная часть вектора будет заполнена нулями. Давайте напишем функцию для этого преобразования.\n",
    "\n",
    "#### Задание: допишите функцию one-hot кодирования последовательности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnahALhiceEl"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(int_words: torch.Tensor, n_labels: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates one-hot representation matrix for a given batch of integer sequences\n",
    "    :param int_words: tensor of ints, which represents current sequence; shape: [batch_size, seq_len]\n",
    "    :param n_labels: vocabulary size (number of unique tokens in data)\n",
    "    :return: one-hot representation of the input tensor; shape: [batch_size, seq_len, n_labels]\n",
    "    \"\"\"\n",
    "    words_one_hot = (\n",
    "        torch.nn.functional.one_hot(int_words, num_classes=n_labels)\n",
    "        .to(torch.float32)\n",
    "        .to(int_words.device)\n",
    "    )\n",
    "    return words_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t4ZQhskXF4xL",
    "outputId": "44c9c770-e0fc-4ad2-c4e8-795572bd6d26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# testing the function\n",
    "test_seq = torch.tensor([[3, 5, 1], [0, 2, 4]])\n",
    "test_one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(test_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YyL91CuceEl"
   },
   "source": [
    "### Сформируем батчи\n",
    "На простом примере батчи будут выглядеть так: мы возьмем закодированные символы и разделим их на несколько последовательностей, заданных параметром `batch_size`. Каждая из наших последовательностей будет иметь длину `seq_length`.\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/sequence_batching@1x.png?raw=1\" width=500px>\n",
    "\n",
    "**1. Отбросим часть текста, чтобы у нас были только полные батчи**\n",
    "\n",
    "Каждый батч содержит $N \\times M$ символов, где $N$ — это количество последовательностей в батче (`batch_size`), а $M$ — длина каждой последовательности (`seq_length`). Затем, чтобы получить общее количество батчей $K$, которое мы можем сделать из последовательности, нужно разделить длину последовательности на количество символов в батче. Когда мы узнаем количество батчей, можно получить общее количество символов, которые нужно сохранить, из последовательности: $N \\times M \\times K$.\n",
    "\n",
    "**2. Разделим текст на $N$ частей**\n",
    "\n",
    "Этот шаг нужен, чтобы мы могли проходить по тексту окном размера `[batch_size, seq_len]`. Его можно реализовать при помощи простого `reshape`.\n",
    "\n",
    "**3. Теперь, когда у нас готова матрица текста, мы можем двигаться по ней окном, чтобы получить батчи**\n",
    "\n",
    "Из каждой позиции окна сформируем обучающие пары `(x, y)` следующим образом: $x$ — это все элементы окна кроме последнего столбца, а $y$ — это все элементы окна кроме первого столбца. Тем самым для каждого токена исходного текста мы будем предсказывать следующий за ним токен.\n",
    "\n",
    "#### Задание: допишите функцию генерации батчей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ECftYejnvpx"
   },
   "outputs": [],
   "source": [
    "def get_batches(\n",
    "    int_words: torch.Tensor, batch_size: int, seq_length: int\n",
    ") -> Iterable[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generates batches from encoded sequence.\n",
    "    :param int_words: tensor of ints, which represents the text; shape: [batch_size, -1]\n",
    "    :param batch_size: number of sequences per batch\n",
    "    :param seq_length: number of encoded chars in a sequence\n",
    "    :return: generator of pairs (x, y); x_shape, y_shape: [batch_size, seq_length - 1]\n",
    "    \"\"\"\n",
    "    # 1. Truncate text, so there are only full batches\n",
    "    window_size = seq_length + 1\n",
    "    symbols_per_batch = batch_size * window_size\n",
    "    batch_count = len(int_words) // symbols_per_batch\n",
    "    int_words_to_use = int_words[: batch_count * symbols_per_batch]\n",
    "\n",
    "    # 2. Reshape into batch_size rows\n",
    "    int_words_to_use = int_words_to_use.reshape((batch_size, -1))\n",
    "\n",
    "    # 3. Iterate through the text matrix\n",
    "    for position in range(0, int_words_to_use.shape[1], window_size):\n",
    "        x = int_words_to_use[:, position : position + seq_length]\n",
    "        # So that for Chapter for C the target is h\n",
    "        y = int_words_to_use[:, position + 1 : position + seq_length + 1]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qtKlLXi1ceEl",
    "outputId": "3ae0c720-2f62-4709-92aa-12d1b8f44501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "tensor([[23, 49, 32, 43, 48, 74,  0, 77, 12, 33],\n",
      "        [43, 43, 74, 76, 77, 32,  8, 76, 77, 22],\n",
      "        [48, 49, 32, 48, 77, 55,  8, 22, 24, 61],\n",
      "        [74, 70, 77, 48, 49, 24, 62,  2, 49, 48],\n",
      "        [74,  0, 63, 77, 49, 74, 77, 49, 32, 76],\n",
      "        [ 0, 63, 77, 70, 49, 24, 77, 70, 32, 22],\n",
      "        [22, 48, 77, 53, 74, 77, 30, 24, 20, 74],\n",
      "        [63, 77, 52, 61, 74, 21, 74, 54, 77, 52]])\n",
      "\n",
      "y:\n",
      "tensor([[49, 32, 43, 48, 74,  0, 77, 12, 33, 33],\n",
      "        [43, 74, 76, 77, 32,  8, 76, 77, 22, 49],\n",
      "        [49, 32, 48, 77, 55,  8, 22, 24, 61, 62],\n",
      "        [70, 77, 48, 49, 24, 62,  2, 49, 48, 46],\n",
      "        [ 0, 63, 77, 49, 74, 77, 49, 32, 76, 77],\n",
      "        [63, 77, 70, 49, 24, 77, 70, 32, 22, 77],\n",
      "        [48, 77, 53, 74, 77, 30, 24, 20, 74,  0],\n",
      "        [77, 52, 61, 74, 21, 74, 54, 77, 52, 61]])\n"
     ]
    }
   ],
   "source": [
    "# testing the function\n",
    "test_batches = get_batches(encoded, 8, 50)\n",
    "test_x, test_y = next(test_batches)\n",
    "assert test_x.shape == test_y.shape\n",
    "print(f\"x:\\n{test_x[:10, :10]}\\n\")\n",
    "print(f\"y:\\n{test_y[:10, :10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeaMa5fKF4xM"
   },
   "source": [
    "### Наконец, подготовим класс датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8P8HmW8F4xM"
   },
   "outputs": [],
   "source": [
    "class AnnaData(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, int_words: torch.Tensor, batch_size: int, seq_length: int):\n",
    "        self.int_words = int_words\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        return get_batches(self.int_words, self.batch_size, self.seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jouxv0L2ceEl"
   },
   "source": [
    "## 2. Имплементация модели\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charRNN.png?raw=1\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7s5eRaoceEl"
   },
   "source": [
    "### Структура модели\n",
    "\n",
    "* Создаём и храним необходимые словари.\n",
    "* Определяем слой [LSTM]((https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)) с помощью инстанса класса `torch.nn.LSTM`, который принимает набор параметров: `input_size` — длина последовательности в батче; `n_hidden` — размер скрытых слоёв; `n_layers` — количество слоёв; `drop_prob` — вероятность дропаута; и `batch_first` — флаг, указывающий на то, что у входных последовательностей размерность батча идёт вдоль нулевой оси.\n",
    "* Определяем слой Dropout с таким же значением `drop_prob`.\n",
    "* Определяем полносвязный слой с набором параметров: размерность ввода — `n_hidden`; размерность выхода — размер словаря.\n",
    "* Наконец, инициализируем веса и начальное скрытое состояние (`self.init_hidden()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPq1EA38rBqn"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unique_tokens: Tuple[str],\n",
    "        n_hidden: int = 256,\n",
    "        n_layers: int = 2,\n",
    "        drop_prob: float = 0.5,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        # create mappings\n",
    "        self.unique_tokens = unique_tokens\n",
    "        self.int2char = dict(enumerate(self.unique_tokens))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "\n",
    "        ## define the LSTM, dropout and fully connected layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=len(unique_tokens),\n",
    "            hidden_size=n_hidden,\n",
    "            num_layers=n_layers,\n",
    "            dropout=drop_prob,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(n_hidden, len(unique_tokens))\n",
    "        self.drop = nn.Dropout(drop_prob)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, hidden: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        # Stack up LSTM outputs using view. You may need to use contiguous to reshape the output.\n",
    "        out = self.drop(out)\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        ## Get the output for classification.\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(\n",
    "        self, batch_size: int, weight_device: torch.device\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Creates two new zero tensors for hidden state and cell state of LSTM\n",
    "        :param batch_size: number of sequences per batch\n",
    "        :param weight_device: torch.device(\"cuda\") for GPU init or torch.device(\"cpu\") for CPU init\n",
    "        :return: tuple of two tensors of shape [n_layers x batch_size x n_hidden]\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (\n",
    "            weight.new(self.n_layers, batch_size, self.n_hidden)\n",
    "            .zero_()\n",
    "            .to(weight_device),\n",
    "            weight.new(self.n_layers, batch_size, self.n_hidden)\n",
    "            .zero_()\n",
    "            .to(weight_device),\n",
    "        )\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Om7WKpduF4xN"
   },
   "outputs": [],
   "source": [
    "class CharRNNModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unique_tokens: Tuple[str],\n",
    "        n_hidden: int = 1024,\n",
    "        n_layers: int = 2,\n",
    "        drop_prob: float = 0.5,\n",
    "        batch_size: int = 128,\n",
    "        seq_length=256,\n",
    "        lr: float = 0.001,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model = CharRNN(unique_tokens, n_hidden, n_layers, drop_prob)\n",
    "        self.hidden = None\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.n_chars = len(unique_tokens)\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def training_step(\n",
    "        self, train_batch: Tuple[torch.Tensor, torch.Tensor]\n",
    "    ) -> torch.Tensor:\n",
    "        x, y = train_batch\n",
    "        x, y = x.squeeze(0), y.squeeze(0)\n",
    "        x = one_hot_encode(x, self.n_chars)\n",
    "\n",
    "        if self.hidden is None:\n",
    "            self.hidden = self.model.init_hidden(self.batch_size, self.device)\n",
    "        self.hidden = tuple([each.data for each in self.hidden])\n",
    "\n",
    "        output, self.hidden = self.model(x, self.hidden)\n",
    "        loss = self.loss(output, y.reshape(self.batch_size * self.seq_length).long())\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IrBRlEPceEl"
   },
   "source": [
    "## 3. Обучение модели\n",
    "\n",
    "По классике, используем оптимизатор Adam и кросс-энтропию. Но без пары особенностей не обойтись:\n",
    "* Во время цикла будем отделять скрытое состояние от его истории, потому что скрытое состояние LSTM является кортежем скрытых состояний.\n",
    "* Будем использовать gradient clipping, чтобы избавиться от взрывающихся градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390,
     "referenced_widgets": [
      "3975f58e270e427a816ea9028e8ceb59",
      "a84e9005197e4aaba80a31d2c04c030a",
      "7d6a54c6f7a04ff99adc572948484da6",
      "d63c4cd72e564a10b1e47322297a72ca",
      "fb672db95d7c447fabb0a9eda176a328",
      "13bad821c7c7403f98d43f01ec270780",
      "3fc657ac7ad340b5a006c27f811e9a5e",
      "40056a5f3f564f2e951b270d04b61bf1",
      "9fd771d03c15439ea9202a55c252da4a",
      "68c42e9538ae4fef8c96a2e6cf119a77",
      "d0d76a6f1b98403da86b8829115b0c9b"
     ]
    },
    "id": "InCNB0ICF4xN",
    "outputId": "ee7c3f63-1951-4108-e15a-8fba4b142b88",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model | CharRNN          | 13.0 M | train\n",
      "1 | loss  | CrossEntropyLoss | 0      | train\n",
      "---------------------------------------------------\n",
      "13.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "13.0 M    Total params\n",
      "52.097    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3975f58e270e427a816ea9028e8ceb59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "train_dataset = AnnaData(encoded, batch_size=128, seq_length=256)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,  # batching is already implemented on our side\n",
    "    shuffle=False,\n",
    "    num_workers=1,  # don't change: it will lead to the wrong implementation\n",
    ")\n",
    "# model\n",
    "char_rnn = CharRNNModule(unique_chars, n_hidden=1024, batch_size=128)\n",
    "# trainer\n",
    "trainer = pl.Trainer(max_epochs=15, gradient_clip_val=1.0, accelerator=\"gpu\", devices=1)\n",
    "trainer.fit(char_rnn, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfZxvNoDceEm"
   },
   "source": [
    "## 4. Применение модели\n",
    "\n",
    "Сперва сохраним обученную модель, чтобы можно было загрузить её позже. В следующей ячейке сохраняются параметры, необходимые для создания той же архитектуры, гиперпараметры скрытого слоя и токены."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q6RXl5VAceEm"
   },
   "outputs": [],
   "source": [
    "net = char_rnn.model\n",
    "checkpoint = {\n",
    "    \"n_hidden\": net.n_hidden,\n",
    "    \"n_layers\": net.n_layers,\n",
    "    \"state_dict\": net.state_dict(),\n",
    "    \"tokens\": net.unique_tokens,\n",
    "}\n",
    "\n",
    "with open(\"rnn_x_epoch.net\", \"wb\") as f:\n",
    "    torch.save(checkpoint, f)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2sJhx5iceEm"
   },
   "source": [
    "### Делаем предсказания\n",
    "\n",
    "Сгенерируем текст! Для предсказания продолжения текста мы передаём в сеть последний символ, она предсказывает следующий символ, который мы снова передаем на вход, получаем ещё один предсказанный символ и так далее. Наши прогнозы основаны на категориальном распределении вероятностей по всем возможным символам. Мы можем ограничить число символов на каждом шаге генерации, чтобы сделать получаемый предсказанный текст более разумным, рассматривая только некоторые, наиболее вероятные символы. С одной стороны, такой подход позволит нам рассматривать не только самую вероятную последовательность с точки зрения прогноза модели. С другой стороны, мы будем работать с ограниченным набором сгенерированных вариантов, поэтому избавимся от совсем уж шумовых прогнозов. Узнать больше можно [здесь](https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEIRW_B2ceEm"
   },
   "outputs": [],
   "source": [
    "def predict_next_char(\n",
    "    model: torch.nn.Module, char: str, h: torch.Tensor = None, top_k: int = None\n",
    ") -> Tuple[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Given a character and a model, predicts next character in the sequence\n",
    "    :param model: model that outputs next token probability distribution\n",
    "    :param char: last character of the sequence to continue generation from\n",
    "    :param h: hidden state of the model\n",
    "    :param top_k: number of most probable tokens to be chosen from\n",
    "    :return: tuple of next character and new hidden state\n",
    "    \"\"\"\n",
    "    # tensor inputs\n",
    "    x = torch.tensor([[model.char2int[char]]])\n",
    "    x = one_hot_encode(x, len(model.unique_tokens))\n",
    "    x = x.to(device)\n",
    "\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    out, h = model(x, h)\n",
    "\n",
    "    # get the character probabilities\n",
    "    p = torch.nn.functional.softmax(out, dim=1).data.cpu()\n",
    "\n",
    "    # get top characters\n",
    "    if top_k is None:\n",
    "        top_ch = torch.arange(len(model.unique_tokens))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "\n",
    "    p.squeeze_()\n",
    "    top_ch.squeeze_()\n",
    "    char = top_ch[torch.multinomial(p / p.sum(), 1)]\n",
    "\n",
    "    return model.int2char[char.item()], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG38j3gQceEm"
   },
   "source": [
    "### Priming и генерирование текста\n",
    "\n",
    "Нужно задать скрытое состояние, чтобы сеть не генерировала произвольные символы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9vpB5gRceEm"
   },
   "outputs": [],
   "source": [
    "def sample(model, size, prime=\"The\", top_k=None):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = model.init_hidden(1, device)\n",
    "    for ch in prime:\n",
    "        char, h = predict_next_char(model, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "\n",
    "    # pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict_next_char(model, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return \"\".join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ys0joturF4xO",
    "outputId": "46800975-c4ac-4638-9086-4676f013feef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter with the particularly people, the\n",
      "starratomen she his brother.\n",
      "\n",
      "\"I hope I\n",
      "don't went to make the mother the coming over the same wouldnets,\" she said.\n",
      "\n",
      "\"This may say that I can't all when.... And it we thought of my friend\n",
      "with a\n",
      "carriage of it?\"\n",
      "\n",
      "\"No otely to see the stop in a little of anything in the clotest--it\n",
      "wish betway to be all to be set and always to delighted that she would come by it. And how counting him and he had\n",
      "been doing it on if the cape, and would\n",
      "never be to be meaning to see it in the morning, in\n",
      "a shame. Hos sister's soon, then almott ago,\" he wanted to himself.\n",
      "\n",
      "\"I have to districuse my son,\" said Stepan Arkadyevitch.\n",
      "\n",
      "\"It means, I don't see her what we\n",
      "say, I don't want to the strett on it,\"-said Anna, \"and if you want to\n",
      "say at the propost, but in the carriage well, and talk a money together.\"\n",
      "\n",
      "\"What she was the people are, in sports is in the same, but I see, it must\n",
      "say that?\"\n",
      "\n",
      "\"I should be through about\n",
      "the compatery?\" she repeated,\n",
      "\"and he can't say what i\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime=\"Chapter\", top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "942mjdQHceEm"
   },
   "source": [
    "### Загрузка чекпоинта и генерация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xt9ldUuSceEm",
    "outputId": "bdb498d6-27ed-4863-b968-cffec43d6206",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said at off that he had been sens the crush of which with the more of their hundred too.\n",
      "\n",
      "The marshal, who had been told\n",
      "him along a footman, with her head that his shoulder at the conversation what the people, all he could to\n",
      "be begon to be in thinking at her father's\n",
      "convinced that she went out of them,\" said Levin, looding the look and whingered in his while--at the same hand that\n",
      "she was that it was in the children in that\n",
      "some stard what with\n",
      "such his she had taken too, but he was not the same at\n",
      "the time he was so much to say what he saw that that this mare there was one\n",
      "of the marshal of the whole triel. The way to her.\n",
      "\n",
      "\"All the maid as there.\".. They woman are said when he was confisinced and a fore of her heart to be as a position, which he had taken her side he felt a wife he could not come to see hir sevenal\n",
      "sort. But the corner the short out of them and said his father ten this work.,\n",
      "and was in chaig at to see his fainher, but that the minute had\n",
      "never but imagine their\n",
      "hain, to be angries a lest at the motien of their money and had no time. But the\n",
      "same time, so interested with her. She\n",
      "was not at once and, as though\n",
      "he had so that it seemed to an answer, he was\n",
      "stooding too, and was as a second stopping the simply significance at the\n",
      "provincie on those agificance which he could no doubt, and any official crupling\n",
      "white he herself, and he seamed to her. The clistent of his\n",
      "handsore had stopped in a carriage of the land, and the\n",
      "stacteror would\n",
      "be at once he was,\n",
      "the memories without the childron and he seeing with the servans, and always had been to could not be\n",
      "all to be as the secretary from out of the peasants, and they\n",
      "could have allowed it to see her that she soon at the tonat of so interesting a legt\n",
      "of the part of\n",
      "her hand, that he went on to all, with hat was so that he had taking\n",
      "it a little starting, but helped her that the\n",
      "carred her house, and the sort of the candes were standed at\n",
      "her face, and she saw himseres, that he had been\n",
      "supposing home\n"
     ]
    }
   ],
   "source": [
    "with open(\"rnn_x_epoch.net\", \"rb\") as f:\n",
    "    checkpoint = torch.load(f)\n",
    "\n",
    "loaded = CharRNN(\n",
    "    checkpoint[\"tokens\"],\n",
    "    n_hidden=checkpoint[\"n_hidden\"],\n",
    "    n_layers=checkpoint[\"n_layers\"],\n",
    ")\n",
    "loaded.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "# sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9hqK5IxBIIG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "13bad821c7c7403f98d43f01ec270780": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3975f58e270e427a816ea9028e8ceb59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a84e9005197e4aaba80a31d2c04c030a",
       "IPY_MODEL_7d6a54c6f7a04ff99adc572948484da6",
       "IPY_MODEL_d63c4cd72e564a10b1e47322297a72ca"
      ],
      "layout": "IPY_MODEL_fb672db95d7c447fabb0a9eda176a328"
     }
    },
    "3fc657ac7ad340b5a006c27f811e9a5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "40056a5f3f564f2e951b270d04b61bf1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68c42e9538ae4fef8c96a2e6cf119a77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d6a54c6f7a04ff99adc572948484da6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40056a5f3f564f2e951b270d04b61bf1",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9fd771d03c15439ea9202a55c252da4a",
      "value": 1
     }
    },
    "9fd771d03c15439ea9202a55c252da4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a84e9005197e4aaba80a31d2c04c030a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13bad821c7c7403f98d43f01ec270780",
      "placeholder": "​",
      "style": "IPY_MODEL_3fc657ac7ad340b5a006c27f811e9a5e",
      "value": "Epoch 14: "
     }
    },
    "d0d76a6f1b98403da86b8829115b0c9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d63c4cd72e564a10b1e47322297a72ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68c42e9538ae4fef8c96a2e6cf119a77",
      "placeholder": "​",
      "style": "IPY_MODEL_d0d76a6f1b98403da86b8829115b0c9b",
      "value": " 60/? [00:55&lt;00:00,  1.08it/s, v_num=2, train_loss=1.270]"
     }
    },
    "fb672db95d7c447fabb0a9eda176a328": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
