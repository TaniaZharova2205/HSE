{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "a6uS25Obt_nD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as Func"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!wget https://github.com/karpathy/char-rnn/raw/refs/heads/master/data/tinyshakespeare/input.txt"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXloRUNJumQm",
    "outputId": "6b400cbd-5bb3-4a66-f9fa-5283da7419a9"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2025-05-24 16:08:00--  https://github.com/karpathy/char-rnn/raw/refs/heads/master/data/tinyshakespeare/input.txt\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt [following]\n",
      "--2025-05-24 16:08:01--  https://raw.githubusercontent.com/karpathy/char-rnn/refs/heads/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt.3’\n",
      "\n",
      "input.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.009s  \n",
      "\n",
      "2025-05-24 16:08:01 (123 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "with open(\"input.txt\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ],
   "metadata": {
    "id": "alrrz63tu0cl"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(text))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdPY5wk1u6tA",
    "outputId": "e32e8a2a-c69f-4552-a065-eb49cb93ddc3"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1115394\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(text[:1000])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EL6Gc6szu78q",
    "outputId": "b0eb79df-74f2-4fa7-9747-2ef53c50303e"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "c2i = {ch: i for i, ch in enumerate(chars)}\n",
    "i2c = {i: ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [c2i[c] for c in s]\n",
    "decode = lambda l: \"\".join([i2c[i] for i in l])"
   ],
   "metadata": {
    "id": "3-JP12Z1vFku"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "encode(\"VASYA\\n!\"), decode(encode(\"VASYA\\n!\"))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zB7JyBJb2eb2",
    "outputId": "3914071e-8cec-4945-cfb0-5d613ff106a7"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([34, 13, 31, 37, 13, 0, 2], 'VASYA\\n!')"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype, data[:100])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZ-daqRb3E_1",
    "outputId": "b01e8ce8-99a9-41df-f64b-7f08f0961765"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1115394]) torch.int64 tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "train = data[: int(0.8 * len(data))]\n",
    "valid = data[int(0.8 * len(data)) :]"
   ],
   "metadata": {
    "id": "DPRDfLap3wZt"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "block_size = 256\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data_out = train if split == \"train\" else valid\n",
    "    index = torch.randint(len(data_out) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_out[i : i + block_size] for i in index])\n",
    "    y = torch.stack([data_out[i + 1 : i + block_size + 1] for i in index])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ],
   "metadata": {
    "id": "ROu-pmxs37YI"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# x: Batch, Time, Features\n",
    "# k: Batch, Time, HeadFeatures"
   ],
   "metadata": {
    "id": "zCLWTnFL6Fib"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "n_embd = 384\n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, F = x.shape\n",
    "        tril = torch.tril(torch.ones(block_size, block_size, device=x.device))\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        attention_weight = q @ k.transpose(-2, -1) / (k.shape[-1] ** 0.5)\n",
    "        attention_weight = attention_weight.masked_fill(\n",
    "            tril[:T, :T] == 0, float(\"-inf\")\n",
    "        )\n",
    "        attention_weight = Func.softmax(attention_weight, dim=-1)\n",
    "        attention_weight = self.dropout(attention_weight)\n",
    "        v = self.value(x)\n",
    "        return attention_weight @ v"
   ],
   "metadata": {
    "id": "sLRnP6694mk-"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for i in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ],
   "metadata": {
    "id": "1565SFHW7v1-"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.feed = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feed(x)"
   ],
   "metadata": {
    "id": "ZAWcG5X183mT"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.multihead = MultiHeadAttention(n_head, head_size)\n",
    "        self.feed = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ln1(x + self.multihead(x))\n",
    "        x = self.ln2(x + self.feed(x))\n",
    "        return x"
   ],
   "metadata": {
    "id": "2Ht3FORK9V-B"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "block_count = 6\n",
    "n_head = 6\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(n_embd, n_head) for i in range(block_count)]\n",
    "        )\n",
    "        self.out = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        token_embd = self.token_embedding(x)  # BTF\n",
    "        position_embd = self.position_embedding(torch.arange(T, device=device))\n",
    "        x = token_embd + position_embd\n",
    "        x = self.blocks(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "wHpA1OYn-G5W"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model = Decoder()\n",
    "model.to(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMTifrEP_o-3",
    "outputId": "87870752-d07b-44b9-da58-e875bb55a802"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (token_embedding): Embedding(65, 384)\n",
       "  (position_embedding): Embedding(256, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (multihead): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed): FeedForward(\n",
       "        (feed): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (multihead): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed): FeedForward(\n",
       "        (feed): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (multihead): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed): FeedForward(\n",
       "        (feed): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (multihead): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed): FeedForward(\n",
       "        (feed): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (multihead): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed): FeedForward(\n",
       "        (feed): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (multihead): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-5): 6 x Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (feed): FeedForward(\n",
       "        (feed): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=384, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sum(p.numel() for p in model.parameters()) / 1e6"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9svBD77T_9P5",
    "outputId": "25d3a71c-37ba-4fdf-8a10-1ca87ed5bb23"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10.788161"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, fused=True)\n",
    "max_iters = 2500\n",
    "eval = 500\n",
    "eval_dur = 50\n",
    "\n",
    "\n",
    "def calc_loss(logits, y):\n",
    "    B, T, F = logits.shape\n",
    "    logits = logits.view(B * T, F)\n",
    "    target = y.view(B * T)\n",
    "    loss = Func.cross_entropy(logits, target)\n",
    "    return loss\n",
    "\n",
    "\n",
    "for i in tqdm(range(max_iters)):\n",
    "    if i % eval == 0 or i == max_iters - 1:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            losses = []\n",
    "            for split in [\"train\", \"eval\"]:\n",
    "                for k in range(eval_dur):\n",
    "                    x, y = get_batch(split)\n",
    "                    logits = model(x)\n",
    "                    loss = calc_loss(logits, y)\n",
    "                    losses.append(loss.detach().item())\n",
    "                print(f\"Step {i} split {split}: {np.mean(losses)}\")\n",
    "            model.train()\n",
    "    x, y = get_batch(\"train\")\n",
    "    logits = model(x)\n",
    "    loss = calc_loss(logits, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfI3u-ShAErz",
    "outputId": "a1872114-28e2-4e55-9a0b-ea1cebb767ae"
   },
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r  0%|          | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 0 split train: 4.3469745063781735\n",
      "Step 0 split eval: 4.344580020904541\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 20%|██        | 500/2500 [04:01<15:00,  2.22it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 500 split train: 1.826581163406372\n",
      "Step 500 split eval: 1.8947253406047821\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 40%|████      | 1000/2500 [08:02<11:16,  2.22it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 1000 split train: 1.484866075515747\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 40%|████      | 1001/2500 [08:19<2:10:59,  5.24s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 1000 split eval: 1.6093757796287536\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 60%|██████    | 1500/2500 [12:03<07:28,  2.23it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 1500 split train: 1.3401396203041076\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 60%|██████    | 1501/2500 [12:20<1:26:25,  5.19s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 1500 split eval: 1.497417219877243\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 80%|████████  | 2000/2500 [16:04<03:44,  2.23it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 2000 split train: 1.2550005960464476\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\r 80%|████████  | 2001/2500 [16:20<42:57,  5.17s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 2000 split eval: 1.4388985979557036\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|█████████▉| 2499/2500 [20:03<00:00,  2.23it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 2499 split train: 1.1949415707588196\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2500/2500 [20:19<00:00,  2.05it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step 2499 split eval: 1.403828316926956\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def generate(primer, max_generation):\n",
    "    for i in range(max_generation):\n",
    "        primer_input = primer[:, -block_size:]\n",
    "        logits = model(primer_input)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = Func.softmax(logits, dim=-1)\n",
    "        next_int = torch.multinomial(probs, num_samples=1)\n",
    "        primer = torch.cat([primer, next_int], dim=-1)\n",
    "    return primer\n",
    "\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(generate(context, max_generation=1000)[0].tolist()))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ifk6B21HCEVJ",
    "outputId": "34a34e31-3e0e-4e16-d5a1-e48bc2678934"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "CORIOLANUS:\n",
      "That od child, in brief, to the trumpest, to\n",
      "hath the crown shore beneful out sucorn in lips.\n",
      "There lawful were yetful love, I at\n",
      "raps, and fell oven to thee show my sove--we did\n",
      "sprithence fit bothes.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "That even with cames med our blood again in a\n",
      "secute, instow her mercy ago.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "My Lord meo's fairl, uncle is despair with me?\n",
      "Did not comfort, thou hence''st no mattering alone\n",
      "Let my father withding with my sweet\n",
      "'Tyieldst unto unfit\n",
      "Against onest out vhooks that braiten'd of the willly\n",
      "Bidined frown us his truththwhath have not hailt think own\n",
      "As I repent the mark of Tyral: behildly, thou art\n",
      "To rose it the budlower in thy hands, purchast\n",
      "Affetch's princes and say, protesed and him him\n",
      "Which sidiety ofn to hurst by hisbance hath be pordician.\n",
      "\n",
      "DUKE OF YORK:\n",
      "Here do fact unto that husband of him.\n",
      "\n",
      "HASTINGS:\n",
      "Sir, a prospersorditation to do do look.\n",
      "Is it thou hast thou with cheerfect I'll kill him?\n",
      "3 lood King HENRY VI\n",
      "\n",
      "EDWARD IV:\n",
      "Thy saint tel\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "71RSUYzuE9OG"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}